# -*- coding: utf-8 -*-
"""multitask_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TzN7NLJx6aG5UWsbVYDGjYQ6BWJaVSrM
"""


import torch
import torch.nn as nn
# from torch.nn import MSELoss
from torch.nn import BCEWithLogitsLoss
from torch.nn import AdaptiveAvgPool2d
from torch.utils.data import WeightedRandomSampler
from torch.utils.data import Dataset, DataLoader, random_split
from torch.nn import AdaptiveAvgPool2d, Linear, CrossEntropyLoss
from torch.cuda.amp import autocast, GradScaler

from torchvision.models import resnet50
from torchvision import transforms, models
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
from transformers import VisualBertModel, AutoTokenizer, CLIPModel, CLIPTokenizer


import os
import requests
import numpy as np
import pandas as pd
import emoji
import re

from PIL import Image
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from io import BytesIO
from PIL import Image
from PIL import UnidentifiedImageError
import matplotlib.pyplot as plt

import seaborn as sns
from sklearn.metrics import precision_recall_curve


from tqdm import tqdm
import gc
gc.collect()
torch.cuda.empty_cache()


# Constants
BATCH_SIZE =32
LEARNING_RATE = 1e-5
NUM_EPOCHS = 15
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Image Transformations
# Augmentations for training
train_transform = transforms.Compose([
  transforms.RandomResizedCrop(224),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# train_transform = transforms.Compose([
#     transforms.RandomResizedCrop(224),
#     transforms.RandomHorizontalFlip(p=0.5),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.RandomRotation(degrees=10),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
# ])

def clean_text(text):
    if pd.isna(text) or text is None:
        return ""
    if not isinstance(text, str):
        text = str(text)
    text = emoji.demojize(text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.lower().strip()

import os
from PIL import Image, UnidentifiedImageError
import torch
from torch.utils.data import Dataset
import pandas as pd

class MyDataset(Dataset):
    def __init__(self, annotations_file, tokenizer, max_length=128, transform=None, shared_drive_path="./img"):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_labels = self.img_labels.dropna(subset=['hate', 'anti_hate'])
        self.img_labels = self.img_labels.reset_index(drop=True)
        self.transform = transform
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.shared_drive_path = shared_drive_path  # Base path for the shared drive

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_url = self.img_labels.loc[idx, 'img_path']
        #img_name = os.path.basename(img_url)  # Extract the image file name
        #img_path = os.path.join(self.shared_drive_path, img_name)  # Construct full path

        #try:
            # Read image locally from shared drive
        #    image = Image.open(img_path).convert("RGB")
        #    if self.transform:
        #        image = self.transform(image)

        img_url = img_url.replace("\\", "/")

        # Fetch the image from the URL (storing the images in github)
        try:
            response = requests.get(img_url, timeout=20)
            response.raise_for_status()

            # Check if the content is an image
            if "image" not in response.headers["Content-Type"]:
                raise ValueError(f"URL {img_url} does not contain an image")

            # Try to open the image using PIL
            image = Image.open(BytesIO(response.content)).convert("RGB")
            valid_image = True
        except (FileNotFoundError, UnidentifiedImageError) as e:
            print(f"Error loading image at {img_path}: {e}")
            # Use a zero-filled tensor as a placeholder
            image = torch.zeros(3, 224, 224)

        # CAN COMMENT OUT IF UNCOMMENT THE ABOVE
        # Apply transformation only if the image is valid and it's a PIL Image
        if valid_image and isinstance(image, Image.Image):
            image = self.transform(image)

        # Combine `text` and `image_text`
        text = clean_text(self.img_labels.loc[idx, 'text'])
        image_text = clean_text(self.img_labels.loc[idx, 'image_text'])
        text = str(text) + " " + str(image_text)

        # Tokenize the text
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
        )

        # Get sentiments
        hate_sentiment = torch.tensor([self.img_labels.loc[idx, 'hate']], dtype=torch.float32)
        anti_hate_sentiment = torch.tensor([self.img_labels.loc[idx, 'anti_hate']], dtype=torch.float32)

        return img_url, image, text, inputs, hate_sentiment, anti_hate_sentiment

# Feature Extractor
class ResNetFeatureExtractor(nn.Module):
    """This class serves as a feature extractor using a ResNet model, specifically ResNet50.
        It extracts features from images and projects them to a specified output size to match
        the input size required by VisualBERT. The forward method processes an image through the ResNet model,
        applies average pooling, flattens the output, and then projects it to the desired output features size."""

    def __init__(self, output_features=1024):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = resnet50(pretrained=True)
        self.features = nn.Sequential(*list(resnet.children())[:-2]) #selects the layers from this model
        self.pool = AdaptiveAvgPool2d((1, 1)) #resizes the output feature maps produced by the convolutional layers of the neural network.
        self.proj = nn.Linear(2048, output_features)  # Project to match VisualBERT input

    def forward(self, x):
        x = self.features(x)
        x = self.pool(x)
        x = x.flatten(1)
        x = self.proj(x)  # Apply projection (a linear projection layer is used to map the image patch “arrays” to patch embedding “vectors”.)
        return x

feature_extractor = ResNetFeatureExtractor().to(DEVICE)

class MultiTaskVisualBERT(nn.Module):
    """Multi-task VisualBERT that jointly predicts hate and anti-hate"""
    def __init__(self, visual_bert_model_name, dropout_rate=0.1):
        super().__init__()
        self.visual_bert = VisualBertModel.from_pretrained(visual_bert_model_name)

        # Shared representation layer
        self.shared_layer = nn.Linear(self.visual_bert.config.hidden_size, 256)
        self.dropout = nn.Dropout(dropout_rate)

        # Task-specific heads
        self.hate_classifier = nn.Linear(256, 1)
        self.anti_hate_classifier = nn.Linear(256, 1)

    def forward(self, input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids):
        # Get shared representations
        outputs = self.visual_bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            visual_embeds=visual_embeds,
            visual_attention_mask=visual_attention_mask,
            visual_token_type_ids=visual_token_type_ids
        )

        pooled_output = outputs.pooler_output
        shared_features = self.shared_layer(pooled_output)
        shared_features = self.dropout(shared_features)

        # Get both predictions
        hate_logits = self.hate_classifier(shared_features)
        anti_hate_logits = self.anti_hate_classifier(shared_features)

        return hate_logits, anti_hate_logits

class MultiTaskCLIP(nn.Module):
    """Multi-task CLIP that jointly predicts hate and anti-hate"""
    def __init__(self, clip_model_name, dropout_rate=0.1):
        super().__init__()
        self.clip_model = CLIPModel.from_pretrained(clip_model_name)

        # Shared representation layer
        self.shared_layer = nn.Linear(1, 64)
        self.dropout = nn.Dropout(dropout_rate)

        # Task-specific heads
        self.hate_classifier = nn.Linear(64, 1)
        self.anti_hate_classifier = nn.Linear(64, 1)

    def forward(self, input_ids, pixel_values):
        # Get CLIP outputs
        outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values)

        # Extract compatibility scores
        logits = outputs.logits_per_image.diagonal().unsqueeze(1)

        # Shared processing
        shared_features = self.shared_layer(logits)
        shared_features = self.dropout(shared_features)

        # Get both predictions
        hate_logits = self.hate_classifier(shared_features)
        anti_hate_logits = self.anti_hate_classifier(shared_features)

        return hate_logits, anti_hate_logits

# prepare image for clip model
def preprocess_images_for_clip(images):
    preprocess = Compose([
        Resize(224, interpolation=Image.BICUBIC),
        CenterCrop(224),
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])
    processed_images = [preprocess(image) if not torch.is_tensor(image) else image for image in images]
    return torch.stack(processed_images)
    
def calculate_metrics(actual, predicted_probs, threshold=0.5):
    """
    Calculate classification metrics.
    
    Args:
        actual: Ground truth labels
        predicted_probs: Predicted probabilities
        threshold: Threshold for binary classification (default: 0.5)
        
    Returns:
        Dictionary with metrics
    """
    import numpy as np
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
    
    # Flatten the actual labels (if in array format)
    if isinstance(actual[0], (list, np.ndarray)):
        actual = np.array([a[0] for a in actual])
    else:
        actual = np.array(actual)
    
    # Convert predicted probabilities to binary predictions
    predicted_labels = (np.array(predicted_probs) >= threshold).astype(int)
    
    # Calculate metrics
    accuracy = accuracy_score(actual, predicted_labels)
    precision = precision_score(actual, predicted_labels, zero_division=0)
    recall = recall_score(actual, predicted_labels, zero_division=0)
    f1 = f1_score(actual, predicted_labels, zero_division=0)
    
    # Weighted metrics
    precision_weighted = precision_score(actual, predicted_labels, average='weighted', zero_division=0)
    recall_weighted = recall_score(actual, predicted_labels, average='weighted', zero_division=0)
    f1_weighted = f1_score(actual, predicted_labels, average='weighted', zero_division=0)
    
    # Macro metrics
    precision_macro = precision_score(actual, predicted_labels, average='macro', zero_division=0)
    recall_macro = recall_score(actual, predicted_labels, average='macro', zero_division=0)
    f1_macro = f1_score(actual, predicted_labels, average='macro', zero_division=0)
    
    # AUC calculation
    try:
        auc = roc_auc_score(actual, predicted_probs)
    except:
        auc = 0.5  # Default value if AUC can't be calculated
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'auc': auc
    }

class MultiTaskModelManager:
    def __init__(self, visual_model, clip_model, feature_extractor, tokenizer, clip_tokenizer,
                 loss_fn_hate, loss_fn_anti_hate, optimizer_visual, optimizer_clip, smoothing=0.1):
        self.visual_model = visual_model
        self.clip_model = clip_model
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.clip_tokenizer = clip_tokenizer

        self.loss_fn_hate = loss_fn_hate
        self.loss_fn_anti_hate = loss_fn_anti_hate

        self.optimizer_visual = optimizer_visual
        self.optimizer_clip = optimizer_clip
        self.scaler = GradScaler()
        self.smoothing = smoothing

        # Loss tracking
        self.train_losses = {'visual': [], 'clip': []}
        self.val_losses = {'visual': [], 'clip': []}

    def smooth_labels(self, labels, smoothing=0.1):
        """Applies label smoothing."""
        return labels * (1 - smoothing) + 0.5 * smoothing

    def train(self, dataloader, epoch_number):
        # Set models to training mode
        self.visual_model.train()
        self.clip_model.train()
        self.feature_extractor.train()

        total_loss_visual = 0.0
        total_loss_clip = 0.0

        for batch, (image_path, images, texts, inputs, hate_sentiments, anti_hate_sentiments) in tqdm(
                enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch_number+1} [Training]"):

            # Prepare labels
            hate_labels = hate_sentiments.float().to(DEVICE)
            anti_hate_labels = anti_hate_sentiments.float().to(DEVICE)
            hate_labels = self.smooth_labels(hate_labels, self.smoothing)
            anti_hate_labels = self.smooth_labels(anti_hate_labels, self.smoothing)

            images = images.to(DEVICE)
            input_ids = inputs['input_ids'].squeeze(1).to(DEVICE)
            attention_mask = inputs['attention_mask'].squeeze(1).to(DEVICE)

            self.optimizer_visual.zero_grad()
            self.optimizer_clip.zero_grad()

            # Extract visual embeddings
            visual_embeds = self.feature_extractor(images).unsqueeze(1)
            visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=DEVICE)

            # Mixed precision training
            with autocast():
                # VisualBERT forward pass - get both predictions
                hate_logits_visual, anti_hate_logits_visual = self.visual_model(
                    input_ids, attention_mask, visual_embeds, visual_attention_mask, None
                )

                # Calculate losses for both tasks
                loss_hate_visual = self.loss_fn_hate(hate_logits_visual, hate_labels)
                loss_anti_hate_visual = self.loss_fn_anti_hate(anti_hate_logits_visual, anti_hate_labels)
                loss_visual = loss_hate_visual + loss_anti_hate_visual

                # CLIP forward pass - get both predictions
                text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(DEVICE)
                hate_logits_clip, anti_hate_logits_clip = self.clip_model(
                    input_ids=text_inputs.input_ids,
                    pixel_values=preprocess_images_for_clip(images)
                )

                # Calculate losses for both tasks
                loss_hate_clip = self.loss_fn_hate(hate_logits_clip, hate_labels)
                loss_anti_hate_clip = self.loss_fn_anti_hate(anti_hate_logits_clip, anti_hate_labels)
                loss_clip = loss_hate_clip + loss_anti_hate_clip

            # Backpropagation
            self.scaler.scale(loss_visual).backward()
            self.scaler.scale(loss_clip).backward()
            self.scaler.step(self.optimizer_visual)
            self.scaler.step(self.optimizer_clip)
            self.scaler.update()

            total_loss_visual += loss_visual.item()
            total_loss_clip += loss_clip.item()

        avg_loss_visual = total_loss_visual / len(dataloader)
        avg_loss_clip = total_loss_clip / len(dataloader)

        self.train_losses['visual'].append(avg_loss_visual)
        self.train_losses['clip'].append(avg_loss_clip)

        return avg_loss_visual, avg_loss_clip

    def validate(self, dataloader):
        # Set models to evaluation mode
        self.visual_model.eval()
        self.clip_model.eval()
        self.feature_extractor.eval()

        total_loss_visual, total_loss_clip = 0.0, 0.0
        all_hate_labels, all_anti_hate_labels = [], []
        all_hate_probs_visual, all_anti_hate_probs_visual = [], []
        all_hate_probs_clip, all_anti_hate_probs_clip = [], []

        with torch.no_grad():
            for _, (image_path, images, texts, inputs, hate_sentiments, anti_hate_sentiments) in tqdm(
                    enumerate(dataloader), total=len(dataloader), desc="Validating"):

                images = images.to(DEVICE)
                input_ids = inputs['input_ids'].squeeze(1).to(DEVICE)
                attention_mask = inputs['attention_mask'].squeeze(1).to(DEVICE)

                hate_labels = hate_sentiments.to(DEVICE)
                anti_hate_labels = anti_hate_sentiments.to(DEVICE)
                hate_labels = self.smooth_labels(hate_labels, self.smoothing)
                anti_hate_labels = self.smooth_labels(anti_hate_labels, self.smoothing)

                # Extract visual embeddings
                visual_embeds = self.feature_extractor(images).unsqueeze(1)
                visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=DEVICE)

                # VisualBERT predictions
                hate_logits_visual, anti_hate_logits_visual = self.visual_model(
                    input_ids, attention_mask, visual_embeds, visual_attention_mask, None
                )

                hate_probs_visual = torch.sigmoid(hate_logits_visual).squeeze()
                anti_hate_probs_visual = torch.sigmoid(anti_hate_logits_visual).squeeze()

                loss_hate_visual = self.loss_fn_hate(hate_logits_visual, hate_labels)
                loss_anti_hate_visual = self.loss_fn_anti_hate(anti_hate_logits_visual, anti_hate_labels)
                loss_visual = loss_hate_visual + loss_anti_hate_visual
                total_loss_visual += loss_visual.item()

                # CLIP predictions
                text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(DEVICE)
                hate_logits_clip, anti_hate_logits_clip = self.clip_model(
                    input_ids=text_inputs.input_ids,
                    pixel_values=preprocess_images_for_clip(images)
                )

                hate_probs_clip = torch.sigmoid(hate_logits_clip).squeeze()
                anti_hate_probs_clip = torch.sigmoid(anti_hate_logits_clip).squeeze()

                loss_hate_clip = self.loss_fn_hate(hate_logits_clip, hate_labels)
                loss_anti_hate_clip = self.loss_fn_anti_hate(anti_hate_logits_clip, anti_hate_labels)
                loss_clip = loss_hate_clip + loss_anti_hate_clip
                total_loss_clip += loss_clip.item()

                # Collect predictions
                all_hate_labels.extend(hate_labels.cpu().numpy().tolist())
                all_anti_hate_labels.extend(anti_hate_labels.cpu().numpy().tolist())
                all_hate_probs_visual.extend(hate_probs_visual.cpu().numpy())
                all_anti_hate_probs_visual.extend(anti_hate_probs_visual.cpu().numpy())
                all_hate_probs_clip.extend(hate_probs_clip.cpu().numpy())
                all_anti_hate_probs_clip.extend(anti_hate_probs_clip.cpu().numpy())

        avg_loss_visual = total_loss_visual / len(dataloader)
        avg_loss_clip = total_loss_clip / len(dataloader)

        return (avg_loss_visual, avg_loss_clip, all_hate_probs_visual, all_anti_hate_probs_visual,
                all_hate_probs_clip, all_anti_hate_probs_clip, all_hate_labels, all_anti_hate_labels)

    def train_and_validate(self, train_loader, val_loader, num_epochs):
        best_loss_visual = float('inf')
        best_loss_clip = float('inf')

        for epoch in range(num_epochs):
            print(f"Epoch {epoch + 1}/{num_epochs}")

            # Train
            train_loss_visual, train_loss_clip = self.train(train_loader, epoch)

            # Validate
            (val_loss_visual, val_loss_clip, val_hate_probs_visual, val_anti_hate_probs_visual,
             val_hate_probs_clip, val_anti_hate_probs_clip, val_hate_labels, val_anti_hate_labels) = self.validate(val_loader)

            self.val_losses['visual'].append(val_loss_visual)
            self.val_losses['clip'].append(val_loss_clip)

            # Save best models
            if val_loss_visual < best_loss_visual:
                best_loss_visual = val_loss_visual
                torch.save(self.visual_model.state_dict(), './models_trained/best_multitask_visual_model.pth')

            if val_loss_clip < best_loss_clip:
                best_loss_clip = val_loss_clip
                torch.save(self.clip_model.state_dict(), './models_trained/best_multitask_clip_model.pth')

            # Print losses
            print(f"Training Losses - VisualBERT: {train_loss_visual:.4f}, CLIP: {train_loss_clip:.4f}")
            print(f"Validation Losses - VisualBERT: {val_loss_visual:.4f}, CLIP: {val_loss_clip:.4f}")

    def evaluate_test_set(self, test_loader):
        # Load best models
        self.visual_model.load_state_dict(torch.load('./models_trained/best_multitask_visual_model.pth'))
        self.clip_model.load_state_dict(torch.load('./models_trained/best_multitask_clip_model.pth'))

        self.visual_model.eval()
        self.clip_model.eval()
        self.feature_extractor.eval()

        data = {
            'image_name': [], 'text': [],
            'actual_hate_label': [], 'actual_anti_hate_label': [],
            'predicted_probs_visualbert_hate': [], 'predicted_probs_visualbert_anti_hate': [],
            'predicted_probs_clip_hate': [], 'predicted_probs_clip_anti_hate': []
        }

        with torch.no_grad():
            for image_path, images, texts, inputs, hate_sentiments, anti_hate_sentiments in test_loader:
                images = images.to(DEVICE)
                input_ids = inputs['input_ids'].squeeze(1).to(DEVICE)
                attention_mask = inputs['attention_mask'].squeeze(1).to(DEVICE)

                # Process images through feature extractor
                visual_embeds = self.feature_extractor(images)
                if len(visual_embeds.shape) == 2:
                    visual_embeds = visual_embeds.unsqueeze(1)
                visual_attention_mask = torch.ones((visual_embeds.size(0), visual_embeds.size(1)), dtype=torch.long, device=DEVICE)
                visual_token_type_ids = torch.zeros(visual_embeds.shape[:-1], dtype=torch.long, device=DEVICE)

                # Get VisualBERT predictions
                hate_logits_visual, anti_hate_logits_visual = self.visual_model(
                    input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids
                )
                hate_probs_visual = torch.sigmoid(hate_logits_visual).squeeze().cpu()
                anti_hate_probs_visual = torch.sigmoid(anti_hate_logits_visual).squeeze().cpu()

                # Get CLIP predictions
                text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(DEVICE)
                hate_logits_clip, anti_hate_logits_clip = self.clip_model(
                    input_ids=text_inputs.input_ids,
                    pixel_values=preprocess_images_for_clip(images)
                )
                hate_probs_clip = torch.sigmoid(hate_logits_clip).squeeze().cpu()
                anti_hate_probs_clip = torch.sigmoid(anti_hate_logits_clip).squeeze().cpu()

                # Store results
                data['image_name'].extend(image_path)
                data['text'].extend(texts)
                data['actual_hate_label'].extend(hate_sentiments.numpy())
                data['actual_anti_hate_label'].extend(anti_hate_sentiments.numpy())
                data['predicted_probs_visualbert_hate'].extend(hate_probs_visual.numpy())
                data['predicted_probs_visualbert_anti_hate'].extend(anti_hate_probs_visual.numpy())
                data['predicted_probs_clip_hate'].extend(hate_probs_clip.numpy())
                data['predicted_probs_clip_anti_hate'].extend(anti_hate_probs_clip.numpy())

        # Calculate metrics for both tasks
        metrics = {
            'VisualBERT_hate': calculate_metrics(data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
            'VisualBERT_anti_hate': calculate_metrics(data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
            'CLIP_hate': calculate_metrics(data['actual_hate_label'], data['predicted_probs_clip_hate']),
            'CLIP_anti_hate': calculate_metrics(data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate'])
        }

        return pd.DataFrame(data), metrics

    def plot_losses(self):
        """Plot training and validation losses"""
        epochs = range(1, len(self.train_losses['visual']) + 1)

        plt.figure(figsize=(12, 5))

        # VisualBERT losses
        plt.subplot(1, 2, 1)
        plt.plot(epochs, self.train_losses['visual'], label='Train')
        plt.plot(epochs, self.val_losses['visual'], label='Validation', linestyle='--')
        plt.title('Multi-Task VisualBERT Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        # CLIP losses
        plt.subplot(1, 2, 2)
        plt.plot(epochs, self.train_losses['clip'], label='Train')
        plt.plot(epochs, self.val_losses['clip'], label='Validation', linestyle='--')
        plt.title('Multi-Task CLIP Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        plt.tight_layout()
        plt.show()

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_csv_path = './hate/merged_final_hate_train_df.csv'
val_csv_path = './hate/merged_final_hate_validation_df.csv'
test_csv_path = './hate/merged_final_hate_test_df.csv'

# Load datasets directly
train_dataset = MyDataset(train_csv_path, tokenizer, transform=train_transform)
val_dataset = MyDataset(val_csv_path, tokenizer, transform=train_transform)
test_dataset = MyDataset(test_csv_path, tokenizer, transform=train_transform)

# Print dataset sizes to confirm loading
print(f"Training Dataset Size: {len(train_dataset)}")
print(f"Validation Dataset Size: {len(val_dataset)}")
print(f"Test Dataset Size: {len(test_dataset)}")

from torch.nn import BCEWithLogitsLoss
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean', pos_weight=None):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.pos_weight = pos_weight  # Optional pos_weight for class imbalance

    def forward(self, logits, targets):
        # Binary Cross-Entropy with optional pos_weight
        BCE_loss = nn.functional.binary_cross_entropy_with_logits(
            logits, targets, weight=self.pos_weight, reduction="none"
        )
        pt = torch.exp(-BCE_loss)  # Probability for targets
        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss

        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            return focal_loss

# Calculate anti_hate class weight for training data
train_df = pd.read_csv(train_csv_path)
num_hate = train_df['hate'].sum()
num_anti_hate = train_df['anti_hate'].sum()

# Calculate weights for hate and anti_hate
num_hate = train_df['hate'].sum()
num_not_hate = len(train_df) - num_hate
pos_weight_hate = num_not_hate / num_hate

num_anti_hate = train_df['anti_hate'].sum()
num_not_anti_hate = len(train_df) - num_anti_hate
pos_weight_anti_hate = num_not_anti_hate / num_anti_hate

print(f" pos_weight_hate: {pos_weight_hate}, pos_weight_anti_hate {pos_weight_anti_hate}")

# Loss function with pos_weight to handle class imbalance

# loss_fn_hate = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_hate).to(DEVICE))
# loss_fn_anti_hate = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_anti_hate).to(DEVICE))

loss_fn_hate = FocalLoss(
    gamma=1.5, alpha=0.5, reduction='mean',
    pos_weight=torch.tensor([pos_weight_hate]).to(DEVICE)
)

loss_fn_anti_hate = FocalLoss(
    gamma=1.5, alpha=0.5, reduction='mean',
    pos_weight=torch.tensor([pos_weight_anti_hate]).to(DEVICE)
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)

print(f"Number of hate: {num_hate}, Number of not hate: {num_not_hate}")
print(f"Number of anti-hate: {num_anti_hate}, Number of not anti-hate: {num_not_anti_hate}")
print(f"Calculated pos_weight_hate: {pos_weight_hate}, pos_weight_anti_hate: {pos_weight_anti_hate}")

def train_multitask():
    """Train multi-task models that jointly predict hate and anti-hate"""

    # Initialize multi-task models
    visual_model = MultiTaskVisualBERT("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    clip_model = MultiTaskCLIP("openai/clip-vit-base-patch32").to(DEVICE)
    clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

    # Initialize optimizers with different learning rates for different components
    optimizer_visual = torch.optim.AdamW([
        {"params": visual_model.hate_classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},
        {"params": visual_model.anti_hate_classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},
        {"params": visual_model.shared_layer.parameters(), "lr": 5e-5, "weight_decay": 1e-4},
        {"params": visual_model.visual_bert.parameters(), "lr": 1e-6, "weight_decay": 1e-4},
    ])

    optimizer_clip = torch.optim.AdamW([
        {"params": clip_model.hate_classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},
        {"params": clip_model.anti_hate_classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},
        {"params": clip_model.shared_layer.parameters(), "lr": 5e-5, "weight_decay": 1e-4},
        {"params": clip_model.clip_model.parameters(), "lr": 1e-6, "weight_decay": 1e-4},
    ])

    # Create multi-task model manager
    model_manager = MultiTaskModelManager(
        visual_model, clip_model, feature_extractor, tokenizer, clip_tokenizer,
        loss_fn_hate, loss_fn_anti_hate, optimizer_visual, optimizer_clip
    )

    # Training loop
    print("Training multi-task models...")
    model_manager.train_and_validate(train_loader, val_loader, NUM_EPOCHS)

    print("Multi-task training done!")
    return model_manager

# Updated test function
def test_multitask():
    """Test multi-task models"""

    # Initialize models (same as training)
    visual_model = MultiTaskVisualBERT("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    clip_model = MultiTaskCLIP("openai/clip-vit-base-patch32").to(DEVICE)
    clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

    # Create dummy optimizers for evaluation
    optimizer_visual = torch.optim.AdamW(visual_model.parameters(), lr=1e-5)
    optimizer_clip = torch.optim.AdamW(clip_model.parameters(), lr=1e-5)

    # Create model manager
    model_manager = MultiTaskModelManager(
        visual_model, clip_model, feature_extractor, tokenizer, clip_tokenizer,
        loss_fn_hate, loss_fn_anti_hate, optimizer_visual, optimizer_clip
    )

    # Evaluate test set
    print("Evaluating multi-task models on test set...")
    data, metrics = model_manager.evaluate_test_set(test_loader)

    # Print results
    print("\nMulti-Task Model Results:")
    for model_name, model_metrics in metrics.items():
        print(f"\n{model_name}:")
        for metric_name, metric_value in model_metrics.items():
            print(f"  {metric_name}: {metric_value:.4f}")

    # Plot losses
    model_manager.plot_losses()

    return data, metrics

if __name__ == "__main__":
    # For multi-task learning, replace train() and test() with:
    train_multitask()
    test_multitask()
