# -*- coding: utf-8 -*-
"""working_visbert_clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nt8_cl_y38kbPqxllCvz2U_4u4MU9F3c
"""

import torch
import torch.nn as nn
# from torch.nn import MSELoss
from torch.nn import BCEWithLogitsLoss
from torch.nn import AdaptiveAvgPool2d
from torch.utils.data import WeightedRandomSampler
from torch.utils.data import Dataset, DataLoader, random_split
from torch.nn import AdaptiveAvgPool2d, Linear, CrossEntropyLoss
from torch.cuda.amp import autocast, GradScaler

from torchvision.models import resnet50
from torchvision import transforms, models
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
from transformers import VisualBertModel, AutoTokenizer, CLIPModel, CLIPTokenizer


import os
import requests
import numpy as np
import pandas as pd
import emoji
import re

from PIL import Image
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from io import BytesIO
from PIL import Image
from PIL import UnidentifiedImageError
import matplotlib.pyplot as plt

import seaborn as sns
from sklearn.metrics import precision_recall_curve


from tqdm import tqdm
import gc
gc.collect()
torch.cuda.empty_cache()




# Constants
BATCH_SIZE =32
LEARNING_RATE = 1e-5
NUM_EPOCHS = 15
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Image Transformations
# Augmentations for training
train_transform = transforms.Compose([
  transforms.RandomResizedCrop(224),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# train_transform = transforms.Compose([
#     transforms.RandomResizedCrop(224),
#     transforms.RandomHorizontalFlip(p=0.5),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.RandomRotation(degrees=10),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
# ])

def clean_text(text):
    if pd.isna(text) or text is None:
        return ""
    if not isinstance(text, str):
        text = str(text)
    text = emoji.demojize(text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.lower().strip()

import os
from PIL import Image, UnidentifiedImageError
import torch
from torch.utils.data import Dataset
import pandas as pd

class MyDataset(Dataset):
    def __init__(self, annotations_file, tokenizer, max_length=128, transform=None, shared_drive_path="./img"):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_labels = self.img_labels.dropna(subset=['hate', 'anti_hate'])
        self.img_labels = self.img_labels.reset_index(drop=True)
        self.transform = transform
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.shared_drive_path = shared_drive_path  # Base path for the shared drive

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_url = self.img_labels.loc[idx, 'img_path']
        #img_name = os.path.basename(img_url)  # Extract the image file name
        #img_path = os.path.join(self.shared_drive_path, img_name)  # Construct full path

        #try:
            # Read image locally from shared drive
        #    image = Image.open(img_path).convert("RGB")
        #    if self.transform:
        #        image = self.transform(image)
        
        img_url = img_url.replace("\\", "/")

        # Fetch the image from the URL (storing the images in github)
        try:
            response = requests.get(img_url, timeout=10)
            response.raise_for_status()

            # Check if the content is an image
            if "image" not in response.headers["Content-Type"]:
                raise ValueError(f"URL {img_url} does not contain an image")

            # Try to open the image using PIL
            image = Image.open(BytesIO(response.content)).convert("RGB")
            valid_image = True
        except (FileNotFoundError, UnidentifiedImageError) as e:
            print(f"Error loading image at {img_path}: {e}")
            # Use a zero-filled tensor as a placeholder
            image = torch.zeros(3, 224, 224)
        
        # CAN COMMENT OUT IF UNCOMMENT THE ABOVE
        # Apply transformation only if the image is valid and it's a PIL Image
        if valid_image and isinstance(image, Image.Image):
            image = self.transform(image)
        
        # Combine `text` and `image_text`
        text = clean_text(self.img_labels.loc[idx, 'text'])
        image_text = clean_text(self.img_labels.loc[idx, 'image_text'])
        text = str(text) + " " + str(image_text)

        # Tokenize the text
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
        )

        # Get sentiments
        hate_sentiment = torch.tensor([self.img_labels.loc[idx, 'hate']], dtype=torch.float32)
        anti_hate_sentiment = torch.tensor([self.img_labels.loc[idx, 'anti_hate']], dtype=torch.float32)

        return img_url, image, text, inputs, hate_sentiment, anti_hate_sentiment

class VisualBertForSentimentClassification(nn.Module):
    """"This class initializes a VisualBERT model for sentiment classification.
    It includes a VisualBERT model, a linear classifier, a dropout layer for regularization,
    and a sigmoid activation function.
    The forward method processes input through the VisualBERT model and
    then through the classifier. It returns logits which can be transformed into
    probabilities using a sigmoid function (commented out in your code)."""
    def __init__(self, visual_bert_model_name, dropout_rate=0.1):

        super().__init__()
        self.visual_bert = VisualBertModel.from_pretrained(visual_bert_model_name)
        # Classifier layer
        self.classifier = nn.Linear(self.visual_bert.config.hidden_size, 1)  # Outputting a single score
        #  Dropout layer to prevent overfitting: randomly set a fraction `dropout_rate` of input units to 0 during training
        self.dropout = nn.Dropout(dropout_rate)
        # Adding a sigmoid activation function- turns row score into probs bn 0 and 1
        self.sigmoid = nn.Sigmoid()

# take inputs and process through visual_bert and then classifier and sigmoid function to get the final probability
    def forward(self, input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids):
        outputs = self.visual_bert(input_ids=input_ids,
                                   attention_mask=attention_mask,
                                   visual_embeds=visual_embeds,
                                   visual_attention_mask=visual_attention_mask,
                                   visual_token_type_ids=visual_token_type_ids)

        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)  # Apply dropout for overfitting

        # Get raw logits
        logits = self.classifier(pooled_output)

        return logits
        # return self.sigmoid(logits) # suitable for regression-like tasks with continuous labels

# Feature Extractor
class ResNetFeatureExtractor(nn.Module):
    """This class serves as a feature extractor using a ResNet model, specifically ResNet50.
        It extracts features from images and projects them to a specified output size to match
        the input size required by VisualBERT. The forward method processes an image through the ResNet model,
        applies average pooling, flattens the output, and then projects it to the desired output features size."""

    def __init__(self, output_features=1024):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = resnet50(pretrained=True)
        self.features = nn.Sequential(*list(resnet.children())[:-2]) #selects the layers from this model
        self.pool = AdaptiveAvgPool2d((1, 1)) #resizes the output feature maps produced by the convolutional layers of the neural network.
        self.proj = nn.Linear(2048, output_features)  # Project to match VisualBERT input

    def forward(self, x):
        x = self.features(x)
        x = self.pool(x)
        x = x.flatten(1)
        x = self.proj(x)  # Apply projection (a linear projection layer is used to map the image patch “arrays” to patch embedding “vectors”.)
        return x

feature_extractor = ResNetFeatureExtractor().to(DEVICE)

# # Threshold Optimization Function #for model performance
# def optimize_threshold(y_true, y_probs):
#     precision, recall, thresholds = precision_recall_curve(y_true, y_probs)
#     f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
#     best_idx = np.argmax(f1_scores)
#     return thresholds[best_idx]

"""This function preprocesses a list of images to be compatible with the CLIP model.
The preprocessing steps include resizing, center cropping, converting to a tensor,
and normalizing using CLIP-specific mean and standard deviation values.
The function handles both PIL images and pre-converted PyTorch tensors.
Processed images are stacked together and returned as a single tensor."""

# prepare image for clip model
def preprocess_images_for_clip(images):
    preprocess = Compose([
        Resize(224, interpolation=Image.BICUBIC),
        CenterCrop(224),
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])
    processed_images = [preprocess(image) if not torch.is_tensor(image) else image for image in images]
    return torch.stack(processed_images)

def calculate_metrics(actual, predicted_probs, threshold=None):
    if threshold is None:
        threshold = 0.5  # Default threshold
    # Flatten the actual labels from the array format
    actual = np.array([a[0] for a in actual])

    # Convert predicted probabilities to binary predictions using the threshold
    predicted_labels = (np.array(predicted_probs) >= threshold).astype(int)

    # Calculate different metrics
    accuracy = accuracy_score(actual, predicted_labels)
    precision = precision_score(actual, predicted_labels,  zero_division=0)
    recall = recall_score(actual, predicted_labels, zero_division=0)
    f1 = f1_score(actual, predicted_labels, zero_division=0)

    # accuracy = accuracy_score(actual, predicted_labels)
    precision_weighted = precision_score(actual, predicted_labels, average='weighted', zero_division=0)
    recall_weighted = recall_score(actual, predicted_labels, average='weighted', zero_division=0)
    f1_weighted = f1_score(actual, predicted_labels, average='weighted', zero_division=0)

    precision_macro = precision_score(actual, predicted_labels, average='macro', zero_division=0)
    recall_macro = recall_score(actual, predicted_labels, average='macro', zero_division=0)
    f1_macro = f1_score(actual, predicted_labels, average='macro', zero_division=0)

    # AUC calculation (requires raw probabilities, not binary labels)
    auc = roc_auc_score(actual, predicted_probs)

    # Return a dictionary with all metrics
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'auc': auc
    }


"""This class integrates the CLIP model for sentiment analysis.
It includes a dropout layer for regularization and a classifier layer consisting of a linear transformation followed by a sigmoid activation function.
The forward method passes input text and pixel values through the CLIP model, extracts the diagonal elements of the output logits (representing image-text compatibility), applies dropout and the classifier, and finally applies sigmoid activation to obtain probabilities.
The probabilities are transformed into sentiment labels (0 or 1) based on a threshold of 0.5."""
# processes the text and images using CLIP, gets a score for each image-text pair, and then converts these scores into probabilities
class CLIPForSentimentAnalysis(nn.Module):
    def __init__(self, clip_model_name, dropout_rate=0.1):
        super(CLIPForSentimentAnalysis, self).__init__()
        self.clip_model = CLIPModel.from_pretrained(clip_model_name)
        self.dropout = nn.Dropout(dropout_rate)

        self.classifier = nn.Linear(1, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, input_ids, pixel_values):
        # Pass inputs through CLIP model
        outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values)

        # Extract the diagonal elements representing the image-text compatibility
        logits = outputs.logits_per_image.diagonal().unsqueeze(1)
        logits = self.dropout(logits)  # Apply dropout before classifier
        # Apply classifier
        logits = self.classifier(logits)
        # Apply sigmoid activation to get probabilities
        probs = self.sigmoid(logits)
        # Convert probabilities to sentiment labels (0 or 1)
        labels = (probs > 0.5).int()

        return logits

# create class that handles validation and training of models
class ModelManager:
    def __init__(self, visual_model_neg, visual_model_pos, clip_model_neg, clip_model_pos, feature_extractor, tokenizer, clip_tokenizer, loss_fn_hate, loss_fn_anti_hate, optimizer_visual_neg, optimizer_visual_pos, optimizer_clip_neg, optimizer_clip_pos, smoothing=0.1):
        # Initialization with models, tokenizers, loss function, and optimizers
        self.visual_model_neg = visual_model_neg
        self.visual_model_pos = visual_model_pos
        self.clip_model_neg = clip_model_neg
        self.clip_model_pos = clip_model_pos
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.clip_tokenizer = clip_tokenizer

        # Loss function and optimizers
        self.loss_fn_hate = loss_fn_hate
        self.loss_fn_anti_hate = loss_fn_anti_hate


        self.optimizer_visual_neg = optimizer_visual_neg
        self.optimizer_visual_pos = optimizer_visual_pos
        self.optimizer_clip_neg = optimizer_clip_neg
        self.optimizer_clip_pos = optimizer_clip_pos
        self.scaler = GradScaler()  # mixed precision scaler
        self.smoothing = smoothing  # label smoothing factor

        # For loss tracking
        self.visual_neg_losses = []
        self.visual_pos_losses = []
        self.clip_neg_losses = []
        self.clip_pos_losses = []
        self.train_losses = {
                'visual_neg': [],
                'visual_pos': [],
                'clip_neg': [],
                'clip_pos': []
            }
        self.val_losses = {
              'visual_neg': [],
              'visual_pos': [],
              'clip_neg': [],
              'clip_pos': []
          }

    def smooth_labels(self, labels, smoothing=0.1):
        """Applies label smoothing."""
        return labels * (1 - smoothing) + 0.5 * smoothing

    def validate(self, dataloader, sentiment_type):
        # Select models and loss function based on sentiment type
        visual_model = self.visual_model_neg if sentiment_type == 'hate' else self.visual_model_pos
        clip_model = self.clip_model_neg if sentiment_type == 'hate' else self.clip_model_pos
        loss_fn = self.loss_fn_hate if sentiment_type == 'hate' else self.loss_fn_anti_hate

        # Set models to evaluation mode
        visual_model.eval()
        clip_model.eval()
        self.feature_extractor.eval()

        total_loss_visual, total_loss_clip = 0.0, 0.0
        all_labels, all_probs_visual, all_probs_clip = [], [], []

        with torch.no_grad():
            for _, (image_path, images, texts, inputs, hate_sentiments, anti_hate_sentiments) in tqdm(
                    enumerate(dataloader), total=len(dataloader), desc=f"Validating {sentiment_type}"):

                # Move inputs to device
                images = images.to(DEVICE)
                input_ids = inputs['input_ids'].squeeze(1).to(DEVICE)
                attention_mask = inputs['attention_mask'].squeeze(1).to(DEVICE)
                labels = (hate_sentiments if sentiment_type == 'hate' else anti_hate_sentiments).to(DEVICE)
                labels = self.smooth_labels(labels, self.smoothing)


                # Extract visual embeddings
                visual_embeds = self.feature_extractor(images).unsqueeze(1)
                visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=DEVICE)

                # VisualBERT predictions
                logits_visual = visual_model(input_ids, attention_mask, visual_embeds, visual_attention_mask, None)
                probs_visual = torch.sigmoid(logits_visual).squeeze()
                loss_visual = loss_fn(logits_visual, labels)
                total_loss_visual += loss_visual.item()
                all_probs_visual.extend(probs_visual.cpu().numpy())

                # CLIP predictions
                text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(DEVICE)
                logits_clip = clip_model(input_ids=text_inputs.input_ids, pixel_values=preprocess_images_for_clip(images))
                probs_clip = torch.sigmoid(logits_clip).squeeze()
                loss_clip = loss_fn(logits_clip, labels)
                total_loss_clip += loss_clip.item()
                all_probs_clip.extend(probs_clip.cpu().numpy())

                # Collect all labels
                all_labels.extend(labels.cpu().numpy().tolist())  # Convert to list and extend

        avg_loss_visual = total_loss_visual / len(dataloader)
        avg_loss_clip = total_loss_clip / len(dataloader)

        return avg_loss_visual, avg_loss_clip, all_probs_visual, all_probs_clip, all_labels


    def train(self, dataloader, sentiment_type, epoch_number):
        # Choose the appropriate models and optimizers
        if sentiment_type == 'hate':
            visual_model = self.visual_model_neg
            clip_model = self.clip_model_neg
            optimizer_visual = self.optimizer_visual_neg
            optimizer_clip = self.optimizer_clip_neg
            loss_fn = self.loss_fn_hate
        else:  # anti_hate
            visual_model = self.visual_model_pos
            clip_model = self.clip_model_pos
            optimizer_visual = self.optimizer_visual_pos
            optimizer_clip = self.optimizer_clip_pos
            loss_fn = self.loss_fn_anti_hate

        # Set models to training mode
        visual_model.train()
        clip_model.train()
        self.feature_extractor.train()

        total_loss_visual = 0.0
        total_loss_clip = 0.0

        # Create a uniform placeholder image (gray color)
        batch_size = dataloader.batch_size
        placeholder_image = torch.ones(1, 3, 224, 224, device=DEVICE) * 0.5  # Gray image
        # Apply normalization to match the expected input format
        placeholder_image = transforms.Normalize(
            mean=[0.485, 0.456, 0.406], 
            std=[0.229, 0.224, 0.225]
        )(placeholder_image)

        for batch, (image_path, images, texts, inputs, hate_sentiments, anti_hate_sentiments) in tqdm(
                enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch_number+1} [Training {sentiment_type}]"):

            # Get sentiment-specific labels
            labels = (hate_sentiments if sentiment_type == 'hate' else anti_hate_sentiments).float().to(DEVICE)
            labels = self.smooth_labels(labels, self.smoothing)

            # Replace real images with placeholder images
            current_batch_size = len(texts)
            uniform_images = placeholder_image.repeat(current_batch_size, 1, 1, 1)  # Repeat for batch size

            #images = images.to(DEVICE)
            input_ids = inputs['input_ids'].squeeze(1).to(DEVICE)
            attention_mask = inputs['attention_mask'].squeeze(1).to(DEVICE)

            optimizer_visual.zero_grad()
            optimizer_clip.zero_grad()

            # Extract embeddings for VisualBERT
            visual_embeds = self.feature_extractor(uniform_images).unsqueeze(1)  # Ensure correct dimensions
            visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=DEVICE)

            # Mixed precision training
            with autocast():
                # VisualBERT forward pass
                logits_visual = visual_model(input_ids, attention_mask, visual_embeds, visual_attention_mask, None)
                loss_visual = loss_fn(logits_visual, labels)

                # CLIP forward pass
                text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(DEVICE)
                logits_clip = clip_model(input_ids=text_inputs.input_ids, pixel_values=preprocess_images_for_clip(uniform_images))
                loss_clip = loss_fn(logits_clip, labels)

            # Backpropagation
            self.scaler.scale(loss_visual).backward()
            self.scaler.scale(loss_clip).backward()
            self.scaler.step(optimizer_visual)
            self.scaler.step(optimizer_clip)
            self.scaler.update()

            total_loss_visual += loss_visual.item()
            total_loss_clip += loss_clip.item()

        avg_loss_visual = total_loss_visual / len(dataloader)
        avg_loss_clip = total_loss_clip / len(dataloader)

        # Append average losses to respective lists
        if sentiment_type == 'hate':
            self.train_losses['visual_neg'].append(avg_loss_visual)
            self.train_losses['clip_neg'].append(avg_loss_clip)
        else:  # anti_hate
            self.train_losses['visual_pos'].append(avg_loss_visual)
            self.train_losses['clip_pos'].append(avg_loss_clip)

        return avg_loss_visual, avg_loss_clip

    def train_and_validate(self, train_loader, val_loader, num_epochs):
        best_loss_visual_neg = float('inf')
        best_loss_clip_neg = float('inf')
        best_loss_visual_pos = float('inf')
        best_loss_clip_pos = float('inf')

        for epoch in range(num_epochs):
            print(f"Epoch {epoch + 1}/{num_epochs}")

            # Train for both sentiments
            train_loss_visual_neg, train_loss_clip_neg = self.train(train_loader, 'hate', epoch)
            train_loss_visual_pos, train_loss_clip_pos = self.train(train_loader, 'anti_hate', epoch)

            # Validate for both sentiments
            val_loss_visual_neg, val_loss_clip_neg, val_probs_visual_neg, val_probs_clip_neg, val_labels_neg = self.validate(val_loader, 'hate')
            val_loss_visual_pos, val_loss_clip_pos, val_probs_visual_pos, val_probs_clip_pos, val_labels_pos = self.validate(val_loader, 'anti_hate')

            # Append validation losses
            self.val_losses['visual_neg'].append(val_loss_visual_neg)
            self.val_losses['clip_neg'].append(val_loss_clip_neg)
            self.val_losses['visual_pos'].append(val_loss_visual_pos)
            self.val_losses['clip_pos'].append(val_loss_clip_pos)

            # Save best models for 'hate'
            if val_loss_visual_neg < best_loss_visual_neg:
                best_loss_visual_neg = val_loss_visual_neg
                torch.save(self.visual_model_neg.state_dict(), './models_trained/best_visual_model_hate_text.pth')

            if val_loss_clip_neg < best_loss_clip_neg:
                best_loss_clip_neg = val_loss_clip_neg
                torch.save(self.clip_model_neg.state_dict(), './models_trained/best_clip_model_hate_text.pth')

            # Save best models for 'anti_hate'
            if val_loss_visual_pos < best_loss_visual_pos:
                best_loss_visual_pos = val_loss_visual_pos
                torch.save(self.visual_model_pos.state_dict(), './models_trained/best_visual_model_anti_hate_text.pth')

            if val_loss_clip_pos < best_loss_clip_pos:
                best_loss_clip_pos = val_loss_clip_pos
                torch.save(self.clip_model_pos.state_dict(), './models_trained/best_clip_model_anti_hate_text.pth')

            # Print losses for tracking
            print(f"Training Losses - VisualBERT hate: {train_loss_visual_neg:.4f}, CLIP hate: {train_loss_clip_neg:.4f}, "
                  f"VisualBERT anti-hate: {train_loss_visual_pos:.4f}, CLIP anti-hate: {train_loss_clip_pos:.4f}")
            print(f"Validation Losses - VisualBERT hate: {val_loss_visual_neg:.4f}, CLIP hate: {val_loss_clip_neg:.4f}, "
                  f"VisualBERT anti-hate: {val_loss_visual_pos:.4f}, CLIP anti-hate: {val_loss_clip_pos:.4f}")

  #     return metrics
    def plot_losses(self):
        """Plots training and validation losses for all models."""
        epochs = range(1, len(self.train_losses['visual_neg']) + 1)

        # Plot for VisualBERT Hate
        plt.figure(figsize=(10, 6))
        plt.plot(epochs, self.train_losses['visual_neg'], label='Train - VisualBERT Hate')
        plt.plot(epochs, self.val_losses['visual_neg'], label='Validation - VisualBERT Hate', linestyle='--')
        plt.title('Loss - VisualBERT Hate')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

        # Plot for VisualBERT Anti-Hate
        plt.figure(figsize=(10, 6))
        plt.plot(epochs, self.train_losses['visual_pos'], label='Train - VisualBERT Anti-Hate')
        plt.plot(epochs, self.val_losses['visual_pos'], label='Validation - VisualBERT Anti-Hate', linestyle='--')
        plt.title('Loss - VisualBERT Anti-Hate')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

        # Plot for CLIP Hate
        plt.figure(figsize=(10, 6))
        plt.plot(epochs, self.train_losses['clip_neg'], label='Train - CLIP Hate')
        plt.plot(epochs, self.val_losses['clip_neg'], label='Validation - CLIP Hate', linestyle='--')
        plt.title('Loss - CLIP Hate')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

        # Plot for CLIP Anti-Hate
        plt.figure(figsize=(10, 6))
        plt.plot(epochs, self.train_losses['clip_pos'], label='Train - CLIP Anti-Hate')
        plt.plot(epochs, self.val_losses['clip_pos'], label='Validation - CLIP Anti-Hate', linestyle='--')
        plt.title('Loss - CLIP Anti-Hate')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

    def evaluate_test_set(model_manager, test_loader):
        # Load the best models
        model_manager.visual_model_neg.load_state_dict(torch.load('./models_trained/best_visual_model_hate_text.pth'))  #update path as needed
        model_manager.visual_model_pos.load_state_dict(torch.load('./models_trained/best_visual_model_anti_hate_text.pth')) #update path as needed
        model_manager.clip_model_neg.load_state_dict(torch.load('./models_trained/best_clip_model_hate_text.pth')) #update path as needed
        model_manager.clip_model_pos.load_state_dict(torch.load('./models_trained/best_clip_model_anti_hate_text.pth'))  #update path as needed

        sigmoid = torch.nn.Sigmoid()  # Define sigmoid function for converting logits to probabilities
        data = {
            'image_name': [], 'text': [], 'actual_anti_hate_label': [], 'actual_hate_label': [],
            'predicted_probs_visualbert_anti_hate': [], 'predicted_probs_visualbert_hate': [],
            'predicted_probs_clip_anti_hate': [], 'predicted_probs_clip_hate': []
        }

        with torch.no_grad():
            for image_path, images, texts, inputs, hate_sentiments, anti_hate_sentiments in test_loader:
                images = images.to(DEVICE)
                input_ids = inputs['input_ids'].squeeze(1).to(DEVICE)
                attention_mask = inputs['attention_mask'].squeeze(1).to(DEVICE)

                # Process images through feature extractor
                visual_embeds = model_manager.feature_extractor(images)
                if len(visual_embeds.shape) == 2:
                    visual_embeds = visual_embeds.unsqueeze(1)
                visual_attention_mask = torch.ones((visual_embeds.size(0), visual_embeds.size(1)), dtype=torch.long, device=DEVICE)

                visual_token_type_ids = torch.zeros(visual_embeds.shape[:-1], dtype=torch.long, device=DEVICE)

                # Get logits from VisualBERT models and convert to probabilities
                logits_visual_neg = model_manager.visual_model_neg(input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids)
                logits_visual_pos = model_manager.visual_model_pos(input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids)
                probs_visual_neg = sigmoid(logits_visual_neg).squeeze().cpu()
                probs_visual_pos = sigmoid(logits_visual_pos).squeeze().cpu()

                # Prepare texts for CLIP model
                text_inputs = model_manager.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(DEVICE)

                # Get logits from CLIP models and convert to probabilities
                logits_clip_neg = model_manager.clip_model_neg(input_ids=text_inputs.input_ids, pixel_values=preprocess_images_for_clip(images))
                logits_clip_pos = model_manager.clip_model_pos(input_ids=text_inputs.input_ids, pixel_values=preprocess_images_for_clip(images))
                probs_clip_neg = sigmoid(logits_clip_neg).squeeze().cpu()
                probs_clip_pos = sigmoid(logits_clip_pos).squeeze().cpu()

                # Append predictions and labels to data
                data['image_name'].extend(image_path)
                data['text'].extend(texts)
                data['actual_anti_hate_label'].extend(anti_hate_sentiments.numpy())
                data['actual_hate_label'].extend(hate_sentiments.numpy())
                data['predicted_probs_visualbert_anti_hate'].extend(probs_visual_pos.numpy())
                data['predicted_probs_visualbert_hate'].extend(probs_visual_neg.numpy())
                data['predicted_probs_clip_anti_hate'].extend(probs_clip_pos.numpy())
                data['predicted_probs_clip_hate'].extend(probs_clip_neg.numpy())
        print(data['predicted_probs_visualbert_anti_hate'])
        print(data['predicted_probs_clip_anti_hate'])
        # Calculate and return evaluation metrics
        metrics = {
            'VisualBERT_anti_hate': calculate_metrics(data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
            'VisualBERT_hate': calculate_metrics(data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
            'CLIP_anti_hate': calculate_metrics(data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate']),
            'CLIP_hate': calculate_metrics(data['actual_hate_label'], data['predicted_probs_clip_hate'])
        }

        return pd.DataFrame(data), metrics, probs_visual_neg, probs_visual_pos

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_csv_path = './hate/merged_final_hate_train_df.csv'
val_csv_path = './hate/merged_final_hate_validation_df.csv'
test_csv_path = './hate/merged_final_hate_test_df.csv'

# Load datasets directly
train_dataset = MyDataset(train_csv_path, tokenizer, transform=train_transform)
val_dataset = MyDataset(val_csv_path, tokenizer, transform=train_transform)
test_dataset = MyDataset(test_csv_path, tokenizer, transform=train_transform)

# Print dataset sizes to confirm loading
print(f"Training Dataset Size: {len(train_dataset)}")
print(f"Validation Dataset Size: {len(val_dataset)}")
print(f"Test Dataset Size: {len(test_dataset)}")

from torch.nn import BCEWithLogitsLoss
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean', pos_weight=None):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.pos_weight = pos_weight  # Optional pos_weight for class imbalance

    def forward(self, logits, targets):
        # Binary Cross-Entropy with optional pos_weight
        BCE_loss = nn.functional.binary_cross_entropy_with_logits(
            logits, targets, weight=self.pos_weight, reduction="none"
        )
        pt = torch.exp(-BCE_loss)  # Probability for targets
        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss

        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            return focal_loss

# class FocalLoss(nn.Module):
#     def __init__(self, gamma=2.0, alpha=None, reduction='mean', pos_weight=None):
#         """
#         Focal Loss for binary classification with support for class imbalance.

#         Parameters:
#         - gamma (float): Focusing parameter for hard-to-classify examples.
#         - alpha (float or None): Weighting factor for positive class (use for class imbalance).
#         - reduction (str): Specifies the reduction to apply to the output ('mean', 'sum', or 'none').
#         - pos_weight (torch.Tensor or None): Weighting for the positive class.
#         """
#         super(FocalLoss, self).__init__()
#         self.gamma = gamma
#         self.alpha = alpha
#         self.reduction = reduction
#         self.pos_weight = pos_weight

#     def forward(self, logits, targets):
#         # Calculate probabilities for positive class
#         prob = torch.sigmoid(logits)
#         pt = prob * targets + (1 - prob) * (1 - targets)  # Probability for true class

#         # Binary Cross-Entropy loss
#         BCE_loss = nn.functional.binary_cross_entropy_with_logits(
#             logits, targets, weight=self.pos_weight, reduction="none"
#         )

#         # Apply Focal Loss scaling
#         focal_loss = ((1 - pt) ** self.gamma) * BCE_loss

#         # Apply alpha weighting (optional)
#         if self.alpha is not None:
#             alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
#             focal_loss = alpha_t * focal_loss

#         # Reduction
#         if self.reduction == 'mean':
#             return focal_loss.mean()
#         elif self.reduction == 'sum':
#             return focal_loss.sum()
#         else:
#             return focal_loss

# Calculate anti_hate class weight for training data
train_df = pd.read_csv(train_csv_path)
num_hate = train_df['hate'].sum()
num_anti_hate = train_df['anti_hate'].sum()

# Calculate weights for hate and anti_hate
num_hate = train_df['hate'].sum()
num_not_hate = len(train_df) - num_hate
pos_weight_hate = num_not_hate / num_hate

num_anti_hate = train_df['anti_hate'].sum()
num_not_anti_hate = len(train_df) - num_anti_hate
pos_weight_anti_hate = num_not_anti_hate / num_anti_hate

print(f" pos_weight_hate: {pos_weight_hate}, pos_weight_anti_hate {pos_weight_anti_hate}")

# Loss function with pos_weight to handle class imbalance

# loss_fn_hate = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_hate).to(DEVICE))
# loss_fn_anti_hate = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_anti_hate).to(DEVICE))

loss_fn_hate = FocalLoss(
    gamma=1.5, alpha=0.5, reduction='mean',
    pos_weight=torch.tensor([pos_weight_hate]).to(DEVICE)
)

loss_fn_anti_hate = FocalLoss(
    gamma=1.5, alpha=0.5, reduction='mean',
    pos_weight=torch.tensor([pos_weight_anti_hate]).to(DEVICE)
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)

print(f"Number of hate: {num_hate}, Number of not hate: {num_not_hate}")
print(f"Number of anti-hate: {num_anti_hate}, Number of not anti-hate: {num_not_anti_hate}")
print(f"Calculated pos_weight_hate: {pos_weight_hate}, pos_weight_anti_hate: {pos_weight_anti_hate}")



def train(test=False):
    df = pd.read_csv('./merged_final_training.csv')

    # Initialize Models for hate and anti_hate Sentiments
    visual_model_neg = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    visual_model_pos = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)

    clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    clip_model_neg = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)
    clip_model_pos = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)

    optimizer_visual_neg = torch.optim.AdamW([
        {"params": visual_model_neg.classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},  # Higher LR for classifier
        {"params": visual_model_neg.visual_bert.parameters(), "lr": 1e-6, "weight_decay": 1e-4},  # Lower LR for base model
    ])

    optimizer_visual_pos = torch.optim.AdamW([
        {"params": visual_model_pos.classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},
        {"params": visual_model_pos.visual_bert.parameters(), "lr": 1e-6, "weight_decay": 1e-4},
    ])

    optimizer_clip_neg = torch.optim.AdamW([
        {"params": clip_model_neg.classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},  # Higher LR for classifier
        {"params": clip_model_neg.clip_model.parameters(), "lr": 1e-6, "weight_decay": 1e-4},  # Lower LR for base model
    ])

    optimizer_clip_pos = torch.optim.AdamW([
        {"params": clip_model_pos.classifier.parameters(), "lr": 1e-4, "weight_decay": 1e-3},
        {"params": clip_model_pos.clip_model.parameters(), "lr": 1e-6, "weight_decay": 1e-4},
    ])

    # Create Model Manager Instance
    model_manager = ModelManager(

        visual_model_neg, visual_model_pos,
        clip_model_neg, clip_model_pos,
        feature_extractor, tokenizer, clip_tokenizer,
        loss_fn_hate, loss_fn_anti_hate,
        optimizer_visual_neg, optimizer_visual_pos, optimizer_clip_neg, optimizer_clip_pos)

    # Training Loop
    for epoch in range(NUM_EPOCHS):
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}\n-------------------------------")
        model_manager.train_and_validate(train_loader, val_loader, 1)

    print("Training done!")
    print("Training complete. Plotting validation loss...")

    model_manager.plot_losses()

    if test == True:
        data, metrics, probs_visual_neg, probs_visual_pos = model_manager.evaluate_test_set(test_loader)
        print(metrics)

def analyze_model_errors(data, threshold=0.5):
    """
    Simple function to analyze error correlation between CLIP and VisualBERT models.
    Add this to your test() function to check if ensembling would be beneficial.
    
    Args:
        data: DataFrame with actual labels and predicted probabilities
        threshold: Threshold for classification (default: 0.5)
        
    Returns:
        None (prints analysis results)
    """
    
    print("\n===== Error Correlation Analysis =====")
    
    # Convert probabilities to binary predictions
    data['visualbert_hate_pred'] = (data['predicted_probs_visualbert_hate'] >= threshold).astype(int)
    data['visualbert_anti_hate_pred'] = (data['predicted_probs_visualbert_anti_hate'] >= threshold).astype(int)
    data['clip_hate_pred'] = (data['predicted_probs_clip_hate'] >= threshold).astype(int)
    data['clip_anti_hate_pred'] = (data['predicted_probs_clip_anti_hate'] >= threshold).astype(int)
    
    # Determine if predictions are correct or errors
    data['visualbert_hate_error'] = (data['visualbert_hate_pred'] != data['actual_hate_label']).astype(int)
    data['visualbert_anti_hate_error'] = (data['visualbert_anti_hate_pred'] != data['actual_anti_hate_label']).astype(int)
    data['clip_hate_error'] = (data['clip_hate_pred'] != data['actual_hate_label']).astype(int)
    data['clip_anti_hate_error'] = (data['clip_anti_hate_pred'] != data['actual_anti_hate_label']).astype(int)
    
    # Calculate error statistics
    results = {}
    for label_type in ['hate', 'anti_hate']:
        vb_errors = data[f'visualbert_{label_type}_error']
        clip_errors = data[f'clip_{label_type}_error']
        
        # Error rates
        vb_error_rate = vb_errors.mean()
        clip_error_rate = clip_errors.mean()
        both_error_rate = (vb_errors & clip_errors).mean()
        either_error_rate = (vb_errors | clip_errors).mean()
        
        # Error correlation
        error_correlation = vb_errors.corr(clip_errors)
        
        # Potential ensemble improvement
        vb_acc = 1 - vb_error_rate
        clip_acc = 1 - clip_error_rate
        best_single_acc = max(vb_acc, clip_acc)
        
        # Theoretical maximum accuracy (if we could always pick the correct model)
        theoretical_max_acc = 1 - both_error_rate
        potential_improvement = theoretical_max_acc - best_single_acc
        
        results[label_type] = {
            'vb_error_rate': vb_error_rate,
            'clip_error_rate': clip_error_rate,
            'both_error_rate': both_error_rate,
            'error_correlation': error_correlation,
            'potential_improvement': potential_improvement,
            'theoretical_max_acc': theoretical_max_acc,
            'best_single_acc': best_single_acc
        }
    
    # Print results
    for label_type in ['hate', 'anti_hate']:
        r = results[label_type]
        print(f"\n{label_type.upper()} Classification:")
        print(f"  Error rates: VisualBERT {r['vb_error_rate']:.4f}, CLIP {r['clip_error_rate']:.4f}")
        print(f"  Both models error rate: {r['both_error_rate']:.4f}")
        print(f"  Error correlation: {r['error_correlation']:.4f} ({'Low' if r['error_correlation'] < 0.3 else 'Moderate' if r['error_correlation'] < 0.6 else 'High'})")
        print(f"  Best single model accuracy: {r['best_single_acc']:.4f}")
        print(f"  Theoretical max ensemble accuracy: {r['theoretical_max_acc']:.4f}")
        print(f"  Potential improvement: {r['potential_improvement']:.4f} ({r['potential_improvement']*100:.2f}%)")
    
    # Calculate average metrics for overall recommendation
    avg_corr = (results['hate']['error_correlation'] + results['anti_hate']['error_correlation']) / 2
    avg_improvement = (results['hate']['potential_improvement'] + results['anti_hate']['potential_improvement']) / 2
    
    print("\n===== Ensemble Recommendation =====")
    if avg_corr < 0.3 and avg_improvement > 0.05:
        print("  ✅ HIGHLY RECOMMENDED: Models make different errors with significant potential improvement")
    elif avg_corr < 0.6 and avg_improvement > 0.02:
        print("  ✅ RECOMMENDED: Moderate error overlap but still worthwhile potential improvement")
    else:
        print("  ❌ NOT RECOMMENDED: High error correlation or limited potential improvement")
    
    # Create visualization
    plt.figure(figsize=(12, 5))
    
    # Error correlation matrix for Hate classification
    plt.subplot(1, 2, 1)
    error_matrix_hate = pd.DataFrame({
        'Model': ['VisualBERT', 'CLIP'],
        'Error Rate': [results['hate']['vb_error_rate'], results['hate']['clip_error_rate']],
        'Correlation': [1, results['hate']['error_correlation']]
    })
    error_corr_matrix = pd.DataFrame(
        [[1, results['hate']['error_correlation']], 
         [results['hate']['error_correlation'], 1]],
        index=['VisualBERT', 'CLIP'],
        columns=['VisualBERT', 'CLIP']
    )
    sns.heatmap(error_corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
    plt.title(f'Hate Error Correlation: {results["hate"]["error_correlation"]:.2f}')
    
    # Error correlation matrix for Anti-Hate classification
    plt.subplot(1, 2, 2)
    error_corr_matrix = pd.DataFrame(
        [[1, results['anti_hate']['error_correlation']], 
         [results['anti_hate']['error_correlation'], 1]],
        index=['VisualBERT', 'CLIP'],
        columns=['VisualBERT', 'CLIP']
    )
    sns.heatmap(error_corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
    plt.title(f'Anti-Hate Error Correlation: {results["anti_hate"]["error_correlation"]:.2f}')
    
    plt.tight_layout()
    plt.savefig('./error_correlation.png')
    print(f"\nError correlation heatmap saved to './error_correlation.png'")
    
    return results


"""# New Section"""
def test():
    # After training
    print("Testing function...")
    """MODEL TESTING"""

    # # Initialize Models for hate and anti_hate Sentiments
    visual_model_neg = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    visual_model_pos = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)

    clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    clip_model_neg = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)
    clip_model_pos = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)

    # # Initialize Optimizers for Each Model
    optimizer_visual_neg = torch.optim.AdamW(visual_model_neg.parameters(), lr=LEARNING_RATE)
    optimizer_visual_pos = torch.optim.AdamW(visual_model_pos.parameters(), lr=LEARNING_RATE)
    optimizer_clip_neg = torch.optim.AdamW(clip_model_neg.parameters(), lr=LEARNING_RATE)
    optimizer_clip_pos = torch.optim.AdamW(clip_model_pos.parameters(), lr=LEARNING_RATE)

    # #  1: Evaluate the test set
    # model_manager = ModelManager(
    #     visual_model_neg, visual_model_pos,
    #     clip_model_neg, clip_model_pos,
    #     feature_extractor, tokenizer, clip_tokenizer,
    #     optimizer_visual_neg, optimizer_visual_pos, optimizer_clip_neg, optimizer_clip_pos)
    # data, metrics, probs_visual_neg, probs_visual_pos = model_manager.evaluate_test_set(test_loader)
    model_manager = ModelManager(
        visual_model_neg, visual_model_pos,
        clip_model_neg, clip_model_pos,
        feature_extractor, tokenizer, clip_tokenizer,
        loss_fn_hate, loss_fn_anti_hate,
         optimizer_visual_neg, optimizer_visual_pos, optimizer_clip_neg, optimizer_clip_pos
    )

    data, metrics, probs_visual_neg, probs_visual_pos = model_manager.evaluate_test_set(test_loader)
    #print(metrics)

    error_results = analyze_model_errors(data)



#import numpy as np
#
#optimal_thresholds = {}
#thresholds = [0.5, 0.6, 0.7, 0.8]
#
## Initialize an empty DataFrame for storing results
#results = []
#models = {
#    "VisualBERT_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
#    "VisualBERT_hate": (data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
#    "CLIP_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate']),
#    "CLIP_hate": (data['actual_hate_label'], data['predicted_probs_clip_hate']),
#}
#
#for model_name, (actual, predicted_probs) in models.items():
#    best_threshold = None
#    best_f1 = 0  # Initialize the best F1 score
#
#    for threshold in thresholds:
#        # Calculate metrics at the given threshold
#        metrics = calculate_metrics(actual, predicted_probs, threshold)
#
#        # Append results to the DataFrame
#        results.append({
#            'Model': model_name,
#            'Threshold': threshold,
#            'Accuracy': metrics['accuracy'],
#            'Precision': metrics['precision'],
#            'Recall': metrics['recall'],
#            'F1': metrics['f1'],
#            'Precision_Weighted': metrics['precision_weighted'],
#            'Recall_Weighted': metrics['recall_weighted'],
#            'F1_Weighted': metrics['f1_weighted'],
#            'Precision_Macro': metrics['precision_macro'],
#            'Recall_Macro': metrics['recall_macro'],
#            'F1_Macro': metrics['f1_macro'],
#            'AUC': metrics['auc']
#        })
#
#        # Update the optimal threshold based on F1 score
#        if metrics['f1'] > best_f1:
#            best_f1 = metrics['f1']
#            best_threshold = threshold
#
#    # Store the optimal threshold for the current model
#    optimal_thresholds[model_name] = best_threshold
#
## Convert results to a DataFrame
#metrics_df = pd.DataFrame(results)
#metrics_df
#
#optimal_thresholds
#
#thresholds = [0.5, 0.6, 0.7, 0.8]
#
#results = []
#models = {
#    "VisualBERT_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
#    "VisualBERT_hate": (data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
#    "CLIP_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate']),
#    "CLIP_hate": (data['actual_hate_label'], data['predicted_probs_clip_hate']),
#}
#
#for model_name, (actual, predicted_probs) in models.items():
#    for threshold in thresholds:
#        metrics = calculate_metrics(actual, predicted_probs, threshold)
#        results.append({
#            'Model': model_name,
#            'Threshold': threshold,
#            'Accuracy': metrics['accuracy'],
#            'Precision': metrics['precision'],
#            'Recall': metrics['recall'],
#            'F1': metrics['f1'],
#            'Precision_Weighted': metrics['precision_weighted'],
#            'Recall_Weighted': metrics['recall_weighted'],
#            'F1_Weighted': metrics['f1_weighted'],
#            'Precision_Macro': metrics['precision_macro'],
#            'Recall_Macro': metrics['recall_macro'],
#            'F1_Macro': metrics['f1_macro'],
#            'AUC': metrics['auc']
#        })
#
#metrics_df = pd.DataFrame(results)
#metrics_df
#
#from sklearn.metrics import precision_recall_curve
#
#precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
#f1_scores = 2 * (precision * recall) / (precision + recall)
#best_threshold = thresholds[np.argmax(f1_scores)]
#
#thresholds = [0.5, 0.6, 0.7, 0.8]
#
#results = []
#models = {
#    "VisualBERT_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
#    "VisualBERT_hate": (data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
#    "CLIP_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate']),
#    "CLIP_hate": (data['actual_hate_label'], data['predicted_probs_clip_hate']),
#}
#
#for model_name, (actual, predicted_probs) in models.items():
#    for threshold in thresholds:
#        metrics = calculate_metrics(actual, predicted_probs, threshold)
#        results.append({
#            'Model': model_name,
#            'Threshold': threshold,
#            'Accuracy': metrics['accuracy'],
#            'Precision': metrics['precision'],
#            'Recall': metrics['recall'],
#            'F1': metrics['f1'],
#            'Precision_Weighted': metrics['precision_weighted'],
#            'Recall_Weighted': metrics['recall_weighted'],
#            'F1_Weighted': metrics['f1_weighted'],
#            'Precision_Macro': metrics['precision_macro'],
#            'Recall_Macro': metrics['recall_macro'],
#            'F1_Macro': metrics['f1_macro'],
#            'AUC': metrics['auc']
#        })
#
#metrics_df = pd.DataFrame(results)
#metrics_df
#
#thresholds = [0.5, 0.6, 0.7, 0.8]
#
#results = []
#models = {
#    "VisualBERT_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
#    "VisualBERT_hate": (data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
#    "CLIP_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate']),
#    "CLIP_hate": (data['actual_hate_label'], data['predicted_probs_clip_hate']),
#}
#
#for model_name, (actual, predicted_probs) in models.items():
#    for threshold in thresholds:
#        metrics = calculate_metrics(actual, predicted_probs, threshold)
#        results.append({
#            'Model': model_name,
#            'Threshold': threshold,
#            'Accuracy': metrics['accuracy'],
#            'Precision': metrics['precision'],
#            'Recall': metrics['recall'],
#            'F1': metrics['f1'],
#            'Precision_Weighted': metrics['precision_weighted'],
#            'Recall_Weighted': metrics['recall_weighted'],
#            'F1_Weighted': metrics['f1_weighted'],
#            'Precision_Macro': metrics['precision_macro'],
#            'Recall_Macro': metrics['recall_macro'],
#            'F1_Macro': metrics['f1_macro'],
#            'AUC': metrics['auc']
#        })
#
#metrics_df = pd.DataFrame(results)
#metrics_df
#
## @title Model vs Threshold
#
#from matplotlib import pyplot as plt
#import seaborn as sns
#figsize = (12, 1.2 * len(metrics_df['Model'].unique()))
#plt.figure(figsize=figsize)
#sns.violinplot(metrics_df, x='Threshold', y='Model', inner='stick', palette='Dark2')
#sns.despine(top=True, right=True, bottom=True, left=True)
#
#thresholds = [0.5, 0.6, 0.7, 0.8]
#
#results = []
#models = {
#    "VisualBERT_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_visualbert_anti_hate']),
#    "VisualBERT_hate": (data['actual_hate_label'], data['predicted_probs_visualbert_hate']),
#    "CLIP_anti_hate": (data['actual_anti_hate_label'], data['predicted_probs_clip_anti_hate']),
#    "CLIP_hate": (data['actual_hate_label'], data['predicted_probs_clip_hate']),
#}
#
#for model_name, (actual, predicted_probs) in models.items():
#    for threshold in thresholds:
#        metrics = calculate_metrics(actual, predicted_probs, threshold)
#        results.append({
#            'Model': model_name,
#            'Threshold': threshold,
#            'Accuracy': metrics['accuracy'],
#            'Precision': metrics['precision'],
#            'Recall': metrics['recall'],
#            'F1': metrics['f1'],
#            'Precision_Weighted': metrics['precision_weighted'],
#            'Recall_Weighted': metrics['recall_weighted'],
#            'F1_Weighted': metrics['f1_weighted'],
#            'Precision_Macro': metrics['precision_macro'],
#            'Recall_Macro': metrics['recall_macro'],
#            'F1_Macro': metrics['f1_macro'],
#            'AUC': metrics['auc']
#        })
#
#metrics_df = pd.DataFrame(results)
#metrics_df
#
#metrics_df[['Model', 'Threshold','Accuracy',	'F1_Macro','Precision_Macro', 'Recall_Macro',	]].sort_values(by=['Model', 'Threshold'])
#
#metrics_df[['Model', 'Threshold','Accuracy',	'F1','Precision', 'Recall',	]].sort_values(by=['Model', 'Threshold'])
#
#metrics_df[['Model', 'Threshold','Accuracy',	'F1_Macro','Precision_Macro', 'Recall_Macro',	]].sort_values(by=['Model', 'Threshold'])
#
## Model	Threshold	Accuracy	F1_Macro	Precision_Macro	Recall_Macro
## 8	CLIP_anti_hate	0.5	0.773607	0.697551	0.696937	0.698179
## 9	CLIP_anti_hate	0.6	0.783578	0.701514	0.708104	0.696097
## 10	CLIP_anti_hate	0.7	0.791202	0.698620	0.718928	0.686119
## 11	CLIP_anti_hate	0.8	0.798240	0.695305	0.733711	0.677334
## 12	CLIP_hate	0.5	0.803519	0.659886	0.638088	0.782986
## 13	CLIP_hate	0.6	0.856891	0.679161	0.655810	0.722953
## 14	CLIP_hate	0.7	0.890909	0.691323	0.697754	0.685479
## 15	CLIP_hate	0.8	0.907918	0.683186	0.755309	0.648795
## 0	VisualBERT_anti_hate	0.5	0.774194	0.703619	0.699796	0.708073
## 1	VisualBERT_anti_hate	0.6	0.780645	0.704618	0.705619	0.703651
## 2	VisualBERT_anti_hate	0.7	0.781232	0.697765	0.704673	0.692161
## 3	VisualBERT_anti_hate	0.8	0.791202	0.700302	0.718804	0.688495
## 4	VisualBERT_hate	0.5	0.824047	0.643717	0.622692	0.704676
## 5	VisualBERT_hate	0.6	0.843988	0.659512	0.637704	0.705517
## 6	VisualBERT_hate	0.7	0.859238	0.668711	0.650244	0.698620
## 7	VisualBERT_hate	0.8	0.866276	0.657638	0.648347	0.669207
#
#import matplotlib.pyplot as plt
#
#plt.figure(figsize=(12, 8))
#
#for model_name in metrics_df['Model'].unique():
#    model_data = metrics_df[metrics_df['Model'] == model_name]
#
#    # Determine style based on "anti_hate" or "hate" in the model name
#    if "hate" in model_name:
#        line_style = "--"  # Dashed line for hate models
#        marker = "+"  # Add dense '+' markers for hate models
#    else:
#        line_style = "-"  # Solid line for anti_hate models
#        marker = None  # No markers for anti_hate models
#
#    # Plot F1 scores with both line and marker (if applicable)
#    plt.plot(
#        model_data['Threshold'],
#        model_data['F1_Weighted'],
#        label=f"{model_name} - F1 Weighted",
#        linestyle=line_style,
#        marker=marker,
#        markersize=8,  # Increase marker size
#        markevery=1  # Plot a marker at every point
#    )
#
## Configure plot
#plt.title("Weighted F1 Scores vs Threshold")
#plt.xlabel("Threshold")
#plt.ylabel("F1 Score")
#plt.legend(loc="lower right")
#plt.grid(True)
#plt.show()
#
#!ls '/content/drive/MyDrive/Memes Project Aim 1/multimodal_models/trained_models'
#
#from sklearn.metrics import roc_curve, auc
#import matplotlib.pyplot as plt
#
#print(f"Model: {model_name}")
#print(f"Unique actual labels: {np.unique(actual)}")
#print(f"Range of predicted probabilities: {predicted_probs.min()} to {predicted_probs.max()}")
#
#
#
#roc_data = {}
#
#for model_name in metrics_df['Model'].unique():
#    actual, predicted_probs = models[model_name]
#
#    # Flatten the `actual` labels and convert `predicted_probs` to a NumPy array
#    actual = np.array([x[0] for x in actual])
#    predicted_probs = np.array(predicted_probs)
#
#    # make sure this is actual is binary and probs are in range
#    if not set(np.unique(actual)).issubset({0, 1}):
#        raise ValueError(f"Actual labels for {model_name} are not binary: {np.unique(actual)}")
#
#    if not (0 <= np.min(predicted_probs) <= np.max(predicted_probs) <= 1):
#        raise ValueError(f"Predicted probabilities for {model_name} are not in range [0, 1]: {np.min(predicted_probs)} to {np.max(predicted_probs)}")
#
#    #  ROC curve and AUC
#    fpr, tpr, _ = roc_curve(actual, predicted_probs)
#    roc_auc = auc(fpr, tpr)
#
#
#    roc_data[model_name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}
#
## Re-plot with a distinction for hate models using a dotted line style
#plt.figure(figsize=(10, 8))
#
#for model_name, roc_info in roc_data.items():
#    fpr = roc_info['fpr']
#    tpr = roc_info['tpr']
#    auc_value = roc_info['auc']
#    # Use a dotted line style for hate models
#    line_style = 'x--' if 'hate' in model_name else '-'
#    plt.plot(fpr, tpr, line_style, label=f"{model_name} (AUC = {auc_value:.2f})")
#
## Plot the diagonal line for random guess
#plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
#
## Add labels, legend, and title
#plt.xlabel('False anti_hate Rate')
#plt.ylabel('True anti_hate Rate')
#plt.title('ROC Curves for Models')
#plt.legend(loc='lower right')
#plt.grid()
#plt.show()



"""# Ensemble

"""

class EnsembleModel:
    """
    Ensemble model that combines CLIP and VisualBERT models for hate and anti-hate classification.
    Provides different ensemble strategies: average, weighted, and learnable weights.
    """
    def __init__(self,
                 visual_model_neg,
                 visual_model_pos,
                 clip_model_neg,
                 clip_model_pos,
                 feature_extractor,
                 tokenizer,
                 clip_tokenizer,
                 device,
                 ensemble_type='weighted',
                 weights=None):
        """
        Initialize the ensemble model.

        Args:
            visual_model_neg: VisualBERT model for hate detection
            visual_model_pos: VisualBERT model for anti-hate detection
            clip_model_neg: CLIP model for hate detection
            clip_model_pos: CLIP model for anti-hate detection
            feature_extractor: Feature extractor for VisualBERT
            tokenizer: Tokenizer for VisualBERT
            clip_tokenizer: Tokenizer for CLIP
            device: Computation device (CPU or CUDA)
            ensemble_type: Type of ensemble - 'average', 'weighted', or 'learnable'
            weights: Initial weights for weighted ensemble (dict with keys for each model)
        """
        self.visual_model_neg = visual_model_neg
        self.visual_model_pos = visual_model_pos
        self.clip_model_neg = clip_model_neg
        self.clip_model_pos = clip_model_pos
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.clip_tokenizer = clip_tokenizer
        self.device = device
        self.ensemble_type = ensemble_type
        self.sigmoid = nn.Sigmoid()

        # Set ensemble weights
        if weights is None:
            if ensemble_type == 'average':
                self.weights = {
                    'visual_neg': 0.5, 'clip_neg': 0.5,
                    'visual_pos': 0.5, 'clip_pos': 0.5
                }
            elif ensemble_type == 'weighted':
                # Default weights based on model performance observed in your metrics
                self.weights = {
                    'visual_neg': 0.55, 'clip_neg': 0.45,
                    'visual_pos': 0.45, 'clip_pos': 0.55
                }
            else:  # learnable
                # Initialize learnable weights as nn.Parameter
                self.learnable_weights = nn.ParameterDict({
                    'hate': nn.Parameter(torch.tensor([0.5, 0.5], device=device)),
                    'anti_hate': nn.Parameter(torch.tensor([0.5, 0.5], device=device))
                })
                self.weight_optimizer = torch.optim.Adam(self.learnable_weights.values(), lr=0.01)
        else:
            self.weights = weights

        # Set models to evaluation mode by default
        self.eval()

    def train(self):
        """Set all models to training mode."""
        self.visual_model_neg.train()
        self.visual_model_pos.train()
        self.clip_model_neg.train()
        self.clip_model_pos.train()
        self.feature_extractor.train()

    def eval(self):
        """Set all models to evaluation mode."""
        self.visual_model_neg.eval()
        self.visual_model_pos.eval()
        self.clip_model_neg.eval()
        self.clip_model_pos.eval()
        self.feature_extractor.eval()

    def preprocess_images_for_clip(self, images):
        """Preprocess images for CLIP model."""
        from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
        from PIL import Image

        preprocess = Compose([
            Resize(224, interpolation=Image.BICUBIC),
            CenterCrop(224),
            ToTensor(),
            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
        ])

        processed_images = [preprocess(image) if not torch.is_tensor(image) else image for image in images]
        return torch.stack(processed_images)

    def forward(self, images, texts, inputs, return_individual=False):
        """
        Forward pass of the ensemble model.

        Args:
            images: Batch of images
            texts: Batch of texts
            inputs: Tokenized inputs for VisualBERT
            return_individual: Whether to return individual model predictions

        Returns:
            Dictionary with hate and anti-hate predictions and optionally individual model outputs
        """
        # Move inputs to device
        images = images.to(self.device)
        input_ids = inputs['input_ids'].squeeze(1).to(self.device)
        attention_mask = inputs['attention_mask'].squeeze(1).to(self.device)

        # Extract visual embeddings for VisualBERT
        visual_embeds = self.feature_extractor(images).unsqueeze(1)
        visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=self.device)

        # VisualBERT predictions
        logits_visual_neg = self.visual_model_neg(input_ids, attention_mask, visual_embeds, visual_attention_mask, None)
        logits_visual_pos = self.visual_model_pos(input_ids, attention_mask, visual_embeds, visual_attention_mask, None)

        # CLIP predictions
        text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(self.device)
        logits_clip_neg = self.clip_model_neg(input_ids=text_inputs.input_ids, pixel_values=self.preprocess_images_for_clip(images))
        logits_clip_pos = self.clip_model_pos(input_ids=text_inputs.input_ids, pixel_values=self.preprocess_images_for_clip(images))

        # Convert logits to probabilities
        probs_visual_neg = self.sigmoid(logits_visual_neg).squeeze()
        probs_visual_pos = self.sigmoid(logits_visual_pos).squeeze()
        probs_clip_neg = self.sigmoid(logits_clip_neg).squeeze()
        probs_clip_pos = self.sigmoid(logits_clip_pos).squeeze()

        # Ensure all probabilities have the right shape
        if len(probs_visual_neg.shape) == 0:
            probs_visual_neg = probs_visual_neg.unsqueeze(0)
        if len(probs_visual_pos.shape) == 0:
            probs_visual_pos = probs_visual_pos.unsqueeze(0)
        if len(probs_clip_neg.shape) == 0:
            probs_clip_neg = probs_clip_neg.unsqueeze(0)
        if len(probs_clip_pos.shape) == 0:
            probs_clip_pos = probs_clip_pos.unsqueeze(0)

        # Combine predictions based on ensemble type
        if self.ensemble_type == 'learnable':
            # Apply softmax to ensure weights sum to 1
            hate_weights = torch.softmax(self.learnable_weights['hate'], dim=0)
            anti_hate_weights = torch.softmax(self.learnable_weights['anti_hate'], dim=0)

            # Weighted sum of predictions
            ensemble_hate = hate_weights[0] * probs_visual_neg + hate_weights[1] * probs_clip_neg
            ensemble_anti_hate = anti_hate_weights[0] * probs_visual_pos + anti_hate_weights[1] * probs_clip_pos
        else:  # 'average' or 'weighted'
            ensemble_hate = (self.weights['visual_neg'] * probs_visual_neg +
                             self.weights['clip_neg'] * probs_clip_neg)
            ensemble_anti_hate = (self.weights['visual_pos'] * probs_visual_pos +
                                  self.weights['clip_pos'] * probs_clip_pos)

        results = {
            'hate_probs': ensemble_hate,
            'anti_hate_probs': ensemble_anti_hate
        }

        if return_individual:
            results.update({
                'visual_neg_probs': probs_visual_neg,
                'visual_pos_probs': probs_visual_pos,
                'clip_neg_probs': probs_clip_neg,
                'clip_pos_probs': probs_clip_pos
            })

        return results

    def train_weights(self, dataloader, loss_fn_hate, loss_fn_anti_hate, num_epochs=5):
        """
        Train the learnable weights for the ensemble model.
        Only applicable when ensemble_type is 'learnable'.

        Args:
            dataloader: DataLoader with training data
            loss_fn_hate: Loss function for hate prediction
            loss_fn_anti_hate: Loss function for anti-hate prediction
            num_epochs: Number of epochs to train the weights
        """
        if self.ensemble_type != 'learnable':
            print("Weight training is only applicable for 'learnable' ensemble type.")
            return

        # Set models to evaluation mode since we're only training the weights
        self.eval()

        for epoch in range(num_epochs):
            total_loss = 0.0

            for _, (_, images, texts, inputs, hate_labels, anti_hate_labels) in enumerate(dataloader):
                # Move labels to device
                hate_labels = hate_labels.to(self.device)
                anti_hate_labels = anti_hate_labels.to(self.device)

                # Get predictions from all models
                with torch.no_grad():
                    # Move inputs to device
                    images = images.to(self.device)
                    input_ids = inputs['input_ids'].squeeze(1).to(self.device)
                    attention_mask = inputs['attention_mask'].squeeze(1).to(self.device)

                    # Extract visual embeddings for VisualBERT
                    visual_embeds = self.feature_extractor(images).unsqueeze(1)
                    visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=self.device)

                    # VisualBERT predictions
                    logits_visual_neg = self.visual_model_neg(input_ids, attention_mask, visual_embeds, visual_attention_mask, None)
                    logits_visual_pos = self.visual_model_pos(input_ids, attention_mask, visual_embeds, visual_attention_mask, None)

                    # CLIP predictions
                    text_inputs = self.clip_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(self.device)
                    logits_clip_neg = self.clip_model_neg(input_ids=text_inputs.input_ids, pixel_values=self.preprocess_images_for_clip(images))
                    logits_clip_pos = self.clip_model_pos(input_ids=text_inputs.input_ids, pixel_values=self.preprocess_images_for_clip(images))

                    # Convert logits to probabilities
                    probs_visual_neg = self.sigmoid(logits_visual_neg)
                    probs_visual_pos = self.sigmoid(logits_visual_pos)
                    probs_clip_neg = self.sigmoid(logits_clip_neg)
                    probs_clip_pos = self.sigmoid(logits_clip_pos)

                # Reset gradients
                self.weight_optimizer.zero_grad()

                # Apply softmax to ensure weights sum to 1
                hate_weights = torch.softmax(self.learnable_weights['hate'], dim=0)
                anti_hate_weights = torch.softmax(self.learnable_weights['anti_hate'], dim=0)

                # Weighted sum of predictions
                ensemble_hate = hate_weights[0] * probs_visual_neg + hate_weights[1] * probs_clip_neg
                ensemble_anti_hate = anti_hate_weights[0] * probs_visual_pos + anti_hate_weights[1] * probs_clip_pos

                # Calculate loss
                loss_hate = loss_fn_hate(ensemble_hate, hate_labels)
                loss_anti_hate = loss_fn_anti_hate(ensemble_anti_hate, anti_hate_labels)
                loss = loss_hate + loss_anti_hate

                # Backpropagation and optimization
                loss.backward()
                self.weight_optimizer.step()

                total_loss += loss.item()

            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}")
            print(f"Hate weights: {torch.softmax(self.learnable_weights['hate'], dim=0).detach().cpu().numpy()}")
            print(f"Anti-hate weights: {torch.softmax(self.learnable_weights['anti_hate'], dim=0).detach().cpu().numpy()}")

    def evaluate(self, dataloader, hate_threshold=0.5, anti_hate_threshold=0.5):
        """
        Evaluate the ensemble model on a dataset.

        Args:
            dataloader: DataLoader with evaluation data
            hate_threshold: Threshold for hate prediction
            anti_hate_threshold: Threshold for anti-hate prediction

        Returns:
            Dictionary with evaluation metrics
        """
        self.eval()

        all_hate_labels = []
        all_anti_hate_labels = []
        all_hate_preds = []
        all_anti_hate_preds = []
        all_hate_probs = []
        all_anti_hate_probs = []

        with torch.no_grad():
            for _, (_, images, texts, inputs, hate_labels, anti_hate_labels) in enumerate(dataloader):
                # Get predictions
                results = self.forward(images, texts, inputs)

                # Convert to binary predictions
                hate_preds = (results['hate_probs'] >= hate_threshold).int()
                anti_hate_preds = (results['anti_hate_probs'] >= anti_hate_threshold).int()

                # Add to lists
                all_hate_labels.extend(hate_labels.cpu().numpy())
                all_anti_hate_labels.extend(anti_hate_labels.cpu().numpy())
                all_hate_preds.extend(hate_preds.cpu().numpy())
                all_anti_hate_preds.extend(anti_hate_preds.cpu().numpy())
                all_hate_probs.extend(results['hate_probs'].cpu().numpy())
                all_anti_hate_probs.extend(results['anti_hate_probs'].cpu().numpy())

        # Calculate metrics
        hate_metrics = self._calculate_metrics(all_hate_labels, all_hate_probs, all_hate_preds)
        anti_hate_metrics = self._calculate_metrics(all_anti_hate_labels, all_anti_hate_probs, all_anti_hate_preds)

        return {
            'hate': hate_metrics,
            'anti_hate': anti_hate_metrics
        }

    def _calculate_metrics(self, labels, probs, preds):
        """Calculate classification metrics."""
        # Flatten the labels and convert to numpy arrays
        labels = np.array([label[0] for label in labels])

        metrics = {
            'accuracy': accuracy_score(labels, preds),
            'precision': precision_score(labels, preds, zero_division=0),
            'recall': recall_score(labels, preds, zero_division=0),
            'f1': f1_score(labels, preds, zero_division=0),
            'auc': roc_auc_score(labels, probs)
        }

        # Add weighted and macro metrics
        metrics.update({
            'precision_weighted': precision_score(labels, preds, average='weighted', zero_division=0),
            'recall_weighted': recall_score(labels, preds, average='weighted', zero_division=0),
            'f1_weighted': f1_score(labels, preds, average='weighted', zero_division=0),
            'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),
            'recall_macro': recall_score(labels, preds, average='macro', zero_division=0),
            'f1_macro': f1_score(labels, preds, average='macro', zero_division=0)
        })

        return metrics

    def save_weights(self, path):
        """Save ensemble weights to a file."""
        if self.ensemble_type == 'learnable':
            weights_dict = {
                'hate': torch.softmax(self.learnable_weights['hate'], dim=0).detach().cpu().numpy(),
                'anti_hate': torch.softmax(self.learnable_weights['anti_hate'], dim=0).detach().cpu().numpy()
            }
        else:
            weights_dict = self.weights

        torch.save(weights_dict, path)

    def load_weights(self, path):
        """Load ensemble weights from a file."""
        weights_dict = torch.load(path,weights_only=False)

        if self.ensemble_type == 'learnable':
            with torch.no_grad():
                # Convert numpy arrays to tensors if needed
                if isinstance(weights_dict['hate'], np.ndarray):
                    hate_weights = torch.tensor(weights_dict['hate'], device=self.device)
                    anti_hate_weights = torch.tensor(weights_dict['anti_hate'], device=self.device)
                else:
                    hate_weights = weights_dict['hate'].to(self.device)
                    anti_hate_weights = weights_dict['anti_hate'].to(self.device)

                # Apply inverse softmax (log) to get the raw parameter values
                self.learnable_weights['hate'].copy_(torch.log(hate_weights))
                self.learnable_weights['anti_hate'].copy_(torch.log(anti_hate_weights))
        else:
            self.weights = weights_dict

    def optimize_thresholds(self, dataloader, threshold_range=None):
        """
        Find optimal thresholds for hate and anti-hate classification.

        Args:
            dataloader: DataLoader with validation data
            threshold_range: Range of thresholds to try (default: [0.1, 0.9, 0.1])

        Returns:
            Dictionary with optimal thresholds and corresponding F1 scores
        """
        if threshold_range is None:
            thresholds = np.arange(0.1, 1.0, 0.1)
        else:
            start, end, step = threshold_range
            thresholds = np.arange(start, end, step)

        best_hate_threshold = 0.5
        best_anti_hate_threshold = 0.5
        best_hate_f1 = 0.0
        best_anti_hate_f1 = 0.0

        self.eval()

        all_hate_labels = []
        all_anti_hate_labels = []
        all_hate_probs = []
        all_anti_hate_probs = []

        with torch.no_grad():
            for _, (_, images, texts, inputs, hate_labels, anti_hate_labels) in enumerate(dataloader):
                # Get predictions
                results = self.forward(images, texts, inputs)

                # Add to lists
                all_hate_labels.extend(hate_labels.cpu().numpy())
                all_anti_hate_labels.extend(anti_hate_labels.cpu().numpy())
                all_hate_probs.extend(results['hate_probs'].cpu().numpy())
                all_anti_hate_probs.extend(results['anti_hate_probs'].cpu().numpy())

        # Convert to numpy arrays
        all_hate_labels = np.array([label[0] for label in all_hate_labels])
        all_anti_hate_labels = np.array([label[0] for label in all_anti_hate_labels])
        all_hate_probs = np.array(all_hate_probs)
        all_anti_hate_probs = np.array(all_anti_hate_probs)

        # Try different thresholds for hate
        for threshold in thresholds:
            hate_preds = (all_hate_probs >= threshold).astype(int)
            f1 = f1_score(all_hate_labels, hate_preds, zero_division=0)

            if f1 > best_hate_f1:
                best_hate_f1 = f1
                best_hate_threshold = threshold

        # Try different thresholds for anti-hate
        for threshold in thresholds:
            anti_hate_preds = (all_anti_hate_probs >= threshold).astype(int)
            f1 = f1_score(all_anti_hate_labels, anti_hate_preds, zero_division=0)

            if f1 > best_anti_hate_f1:
                best_anti_hate_f1 = f1
                best_anti_hate_threshold = threshold

        return {
            'hate_threshold': best_hate_threshold,
            'anti_hate_threshold': best_anti_hate_threshold,
            'hate_f1': best_hate_f1,
            'anti_hate_f1': best_anti_hate_f1
        }

# Constants (from your original code)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 32

def train_and_evaluate_ensemble():
    # Load your pretrained models (use the paths where you saved your best models)
    visual_model_neg = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    visual_model_pos = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    clip_model_neg = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)
    clip_model_pos = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)

    # Load state dictionaries
    visual_model_neg.load_state_dict(torch.load('./models_trained/best_visual_model_hate.pth'))
    visual_model_pos.load_state_dict(torch.load('./models_trained/best_visual_model_anti_hate.pth'))
    clip_model_neg.load_state_dict(torch.load('./models_trained/best_clip_model_hate.pth'))
    clip_model_pos.load_state_dict(torch.load('./models_trained/best_clip_model_anti_hate.pth'))

    # Initialize feature extractor
    feature_extractor = ResNetFeatureExtractor().to(DEVICE)

    # Tokenizers
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

    # Create loss functions (same as in your original code)
    train_df = pd.read_csv('./hate/merged_final_hate_train_df.csv')

    # Calculate weights for hate and anti_hate
    num_hate = train_df['hate'].sum()
    num_not_hate = len(train_df) - num_hate
    pos_weight_hate = num_not_hate / num_hate

    num_anti_hate = train_df['anti_hate'].sum()
    num_not_anti_hate = len(train_df) - num_anti_hate
    pos_weight_anti_hate = num_not_anti_hate / num_anti_hate

    loss_fn_hate = FocalLoss(
        gamma=1.5, alpha=0.5, reduction='mean',
        pos_weight=torch.tensor([pos_weight_hate]).to(DEVICE)
    )

    loss_fn_anti_hate = FocalLoss(
        gamma=1.5, alpha=0.5, reduction='mean',
        pos_weight=torch.tensor([pos_weight_anti_hate]).to(DEVICE)
    )

    # Load datasets
    train_csv_path = './hate/merged_final_hate_train_df.csv'
    val_csv_path = './hate/merged_final_hate_validation_df.csv'
    test_csv_path = './hate/merged_final_hate_test_df.csv'

    # Define transform (same as in your original code)
    train_transform = transforms.Compose([
      transforms.RandomResizedCrop(224),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Load datasets
    train_dataset = MyDataset(train_csv_path, tokenizer, transform=train_transform)
    val_dataset = MyDataset(val_csv_path, tokenizer, transform=train_transform)
    test_dataset = MyDataset(test_csv_path, tokenizer, transform=train_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    # Try different ensemble strategies and compare results
    ensemble_types = ['average', 'weighted', 'learnable']
    results = {}

    for ensemble_type in ensemble_types:
        print(f"\nTraining and evaluating {ensemble_type} ensemble...")

        # Create ensemble model
        ensemble = EnsembleModel(
            visual_model_neg=visual_model_neg,
            visual_model_pos=visual_model_pos,
            clip_model_neg=clip_model_neg,
            clip_model_pos=clip_model_pos,
            feature_extractor=feature_extractor,
            tokenizer=tokenizer,
            clip_tokenizer=clip_tokenizer,
            device=DEVICE,
            ensemble_type=ensemble_type
        )

        # For learnable weights, train the weights
        if ensemble_type == 'learnable':
            print("Training learnable weights...")
            ensemble.train_weights(train_loader, loss_fn_hate, loss_fn_anti_hate, num_epochs=10)

            # Save trained weights
            ensemble.save_weights(f'./models/ensemble_{ensemble_type}_weights.pth')

        # Find optimal thresholds using validation set
        print("Finding optimal thresholds...")
        thresholds = ensemble.optimize_thresholds(val_loader, threshold_range=(0.3, 0.9, 0.05))
        print(f"Optimal thresholds: {thresholds}")

        # Evaluate on test set
        print("Evaluating on test set...")
        metrics = ensemble.evaluate(
            test_loader,
            hate_threshold=thresholds['hate_threshold'],
            anti_hate_threshold=thresholds['anti_hate_threshold']
        )

        # Store results
        results[ensemble_type] = {
            'thresholds': thresholds,
            'metrics': metrics
        }

        print(f"Hate metrics for {ensemble_type} ensemble:")
        for metric, value in metrics['hate'].items():
            print(f"  {metric}: {value:.4f}")

        print(f"Anti-hate metrics for {ensemble_type} ensemble:")
        for metric, value in metrics['anti_hate'].items():
            print(f"  {metric}: {value:.4f}")

    # Compare results
    compare_results(results)

    # Return the best ensemble model
    best_ensemble_type = find_best_ensemble(results)
    print(f"\nBest ensemble type: {best_ensemble_type}")

    return results, best_ensemble_type

def find_best_ensemble(results):
    """Find the best ensemble type based on F1 scores."""
    best_type = None
    best_score = 0.0

    for ensemble_type, result in results.items():
        # Average the F1 scores for hate and anti-hate
        hate_f1 = result['metrics']['hate']['f1_macro']
        anti_hate_f1 = result['metrics']['anti_hate']['f1_macro']
        avg_f1 = (hate_f1 + anti_hate_f1) / 2

        if avg_f1 > best_score:
            best_score = avg_f1
            best_type = ensemble_type

    return best_type

def compare_results(results):
    """Compare the results of different ensemble types."""
    # Create a DataFrame for comparison
    comparison = []

    for ensemble_type, result in results.items():
        comparison.append({
            'Ensemble Type': ensemble_type,
            'Hate F1': result['metrics']['hate']['f1'],
            'Hate F1 Macro': result['metrics']['hate']['f1_macro'],
            'Hate F1 Weighted': result['metrics']['hate']['f1_weighted'],
            'Hate AUC': result['metrics']['hate']['auc'],
            'Anti-Hate F1': result['metrics']['anti_hate']['f1'],
            'Anti-Hate F1 Macro': result['metrics']['anti_hate']['f1_macro'],
            'Anti-Hate F1 Weighted': result['metrics']['anti_hate']['f1_weighted'],
            'Anti-Hate AUC': result['metrics']['anti_hate']['auc'],
            'Hate Threshold': result['thresholds']['hate_threshold'],
            'Anti-Hate Threshold': result['thresholds']['anti_hate_threshold']
        })

    comparison_df = pd.DataFrame(comparison)
    print("\nEnsemble Comparison:")
    print(comparison_df)

    # Plot comparison
    plt.figure(figsize=(12, 6))

    # Prepare data for plotting
    plot_data = []
    for ensemble_type, result in results.items():
        plot_data.append({
            'Ensemble': ensemble_type,
            'Metric': 'Hate F1',
            'Value': result['metrics']['hate']['f1']
        })
        plot_data.append({
            'Ensemble': ensemble_type,
            'Metric': 'Anti-Hate F1',
            'Value': result['metrics']['anti_hate']['f1']
        })
        plot_data.append({
            'Ensemble': ensemble_type,
            'Metric': 'Hate AUC',
            'Value': result['metrics']['hate']['auc']
        })
        plot_data.append({
            'Ensemble': ensemble_type,
            'Metric': 'Anti-Hate AUC',
            'Value': result['metrics']['anti_hate']['auc']
        })

    plot_df = pd.DataFrame(plot_data)

    # Create grouped bar chart
    sns.barplot(x='Ensemble', y='Value', hue='Metric', data=plot_df)
    plt.title('Ensemble Performance Comparison')
    plt.ylabel('Score')
    plt.ylim(0, 1)
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.savefig('./ensemble_comparison.png')
    plt.show()

def perform_error_analysis(best_ensemble, test_loader):
    """Analyze where the ensemble model is making errors."""
    # Get predictions and labels
    all_image_paths = []
    all_texts = []
    all_hate_labels = []
    all_anti_hate_labels = []
    all_predictions = []

    best_ensemble.eval()

    with torch.no_grad():
        for image_path, images, texts, inputs, hate_labels, anti_hate_labels in test_loader:
            # Get predictions
            results = best_ensemble.forward(images, texts, inputs, return_individual=True)

            # Convert to binary predictions using optimal thresholds
            hate_preds = (results['hate_probs'] >= best_ensemble.thresholds['hate_threshold']).int()
            anti_hate_preds = (results['anti_hate_probs'] >= best_ensemble.thresholds['anti_hate_threshold']).int()

            # Add to lists
            all_image_paths.extend(image_path)
            all_texts.extend(texts)
            all_hate_labels.extend(hate_labels.cpu().numpy())
            all_anti_hate_labels.extend(anti_hate_labels.cpu().numpy())
            all_predictions.append({
                'hate_pred': hate_preds.cpu().numpy(),
                'anti_hate_pred': anti_hate_preds.cpu().numpy(),
                'visualbert_hate': results['visual_neg_probs'].cpu().numpy(),
                'visualbert_anti_hate': results['visual_pos_probs'].cpu().numpy(),
                'clip_hate': results['clip_neg_probs'].cpu().numpy(),
                'clip_anti_hate': results['clip_pos_probs'].cpu().numpy(),
                'ensemble_hate': results['hate_probs'].cpu().numpy(),
                'ensemble_anti_hate': results['anti_hate_probs'].cpu().numpy(),
            })

    # Create DataFrame for analysis
    error_df = pd.DataFrame({
        'image_path': all_image_paths,
        'text': all_texts,
        'hate_label': [label[0] for label in all_hate_labels],
        'anti_hate_label': [label[0] for label in all_anti_hate_labels],
        'hate_pred': [pred['hate_pred'][0] for pred in all_predictions],
        'anti_hate_pred': [pred['anti_hate_pred'][0] for pred in all_predictions],
        'visualbert_hate_prob': [pred['visualbert_hate'][0] for pred in all_predictions],
        'visualbert_anti_hate_prob': [pred['visualbert_anti_hate'][0] for pred in all_predictions],
        'clip_hate_prob': [pred['clip_hate'][0] for pred in all_predictions],
        'clip_anti_hate_prob': [pred['clip_anti_hate'][0] for pred in all_predictions],
        'ensemble_hate_prob': [pred['ensemble_hate'][0] for pred in all_predictions],
        'ensemble_anti_hate_prob': [pred['ensemble_anti_hate'][0] for pred in all_predictions],
    })

    # Identify errors
    error_df['hate_error'] = error_df['hate_label'] != error_df['hate_pred']
    error_df['anti_hate_error'] = error_df['anti_hate_label'] != error_df['anti_hate_pred']
    error_df['any_error'] = error_df['hate_error'] | error_df['anti_hate_error']

    # Analyze error patterns
    print("\nError Analysis:")
    print(f"Total samples: {len(error_df)}")
    print(f"Hate classification errors: {error_df['hate_error'].sum()} ({error_df['hate_error'].mean():.2%})")
    print(f"Anti-hate classification errors: {error_df['anti_hate_error'].sum()} ({error_df['anti_hate_error'].mean():.2%})")
    print(f"At least one error: {error_df['any_error'].sum()} ({error_df['any_error'].mean():.2%})")

    # Save error analysis to CSV
    error_df.to_csv('/content/drive/MyDrive/ensemble_error_analysis.csv', index=False)

    return error_df

def ensemble():
    results, best_ensemble_type = train_and_evaluate_ensemble()

    print(f"\nSummary:")
    print(f"Best ensemble type: {best_ensemble_type}")
    print(f"Hate F1: {results[best_ensemble_type]['metrics']['hate']['f1']:.4f}")
    print(f"Anti-Hate F1: {results[best_ensemble_type]['metrics']['anti_hate']['f1']:.4f}")
    print(f"Average F1: {(results[best_ensemble_type]['metrics']['hate']['f1'] + results[best_ensemble_type]['metrics']['anti_hate']['f1']) / 2:.4f}")

    # Compare with individual models
    print("\nComparison with individual models:")
    print("Ensemble vs. Original Models (from metrics_df):")
    print("Model\tHate F1\tAnti-Hate F1")
    print(f"Best Ensemble\t{results[best_ensemble_type]['metrics']['hate']['f1']:.4f}\t{results[best_ensemble_type]['metrics']['anti_hate']['f1']:.4f}")

##### INTERPRETABILITY #####################
import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
import cv2
from PIL import Image
import torch.nn.functional as F
from captum.attr import GradientShap, IntegratedGradients, Occlusion
from transformers import VisualBertModel, CLIPModel
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler


class ModelInterpretability:
    """
    Class for interpreting model predictions on images and text.
    Provides visualization methods for understanding model focus areas.
    """
    def __init__(self, ensemble_model, device):
        """
        Initialize the interpretability class with an ensemble model.
        
        Args:
            ensemble_model: The trained ensemble model
            device: Device for computation (CPU or CUDA)
        """
        self.ensemble_model = ensemble_model
        self.device = device
        
        # Set models in evaluation mode
        self.ensemble_model.eval()
    
    def visualize_image_attention(self, image, text, inputs, model_type='visualbert', attribution_method='gradient', 
                                 target_class='hate', num_steps=50, n_samples=50):
        """
        Visualize where the model is focusing on in the image.
        
        Args:
            image: Input image (torch.Tensor)
            text: Input text
            inputs: Tokenized inputs
            model_type: Which model to interpret ('visualbert' or 'clip')
            attribution_method: Method for generating attribution ('gradient', 'occlusion', or 'shap')
            target_class: Target class for interpretation ('hate' or 'anti_hate')
            num_steps: Number of steps for IntegratedGradients
            n_samples: Number of samples for GradientShap
            
        Returns:
            Original image and heatmap overlay
        """
        # Choose the model for interpretation
        if model_type == 'visualbert':
            if target_class == 'hate':
                model = self.ensemble_model.visual_model_neg
            else:
                model = self.ensemble_model.visual_model_pos
        else:  # CLIP
            if target_class == 'hate':
                model = self.ensemble_model.clip_model_neg
            else:
                model = self.ensemble_model.clip_model_pos
        
        # Prepare inputs
        image = image.unsqueeze(0).to(self.device)  # Add batch dimension
        input_ids = inputs['input_ids'].squeeze(1).to(self.device)
        attention_mask = inputs['attention_mask'].squeeze(1).to(self.device)
        
        # Extract visual embeddings for VisualBERT
        visual_embeds = self.ensemble_model.feature_extractor(image).unsqueeze(1)
        visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=self.device)
        
        # Choose attribution method
        if attribution_method == 'gradient':
            attributions = self._get_integrated_gradients(model, image, input_ids, attention_mask, 
                                                         visual_embeds, visual_attention_mask, model_type, 
                                                         target_class, num_steps)
        elif attribution_method == 'occlusion':
            attributions = self._get_occlusion(model, image, input_ids, attention_mask, 
                                              visual_embeds, visual_attention_mask, model_type, 
                                              target_class)
        elif attribution_method == 'shap':
            attributions = self._get_gradient_shap(model, image, input_ids, attention_mask, 
                                                  visual_embeds, visual_attention_mask, model_type, 
                                                  target_class, n_samples)
        else:
            raise ValueError(f"Unsupported attribution method: {attribution_method}")
        
        # Normalize attributions
        attributions = attributions.detach().cpu().numpy()[0]  # Remove batch dimension
        attributions = np.transpose(attributions, (1, 2, 0))  # Change from CxHxW to HxWxC
        
        # Calculate attribution magnitude across color channels
        attribution_magnitude = np.sqrt(np.sum(attributions**2, axis=2))
        
        # Normalize for visualization
        scaler = MinMaxScaler()
        flattened_magnitude = attribution_magnitude.flatten().reshape(-1, 1)
        normalized_magnitude = scaler.fit_transform(flattened_magnitude).reshape(attribution_magnitude.shape)
        
        # Create heatmap
        return self._create_heatmap(image[0].detach().cpu(), normalized_magnitude, alpha=0.5)
    
    def visualize_text_attention(self, text, inputs, model_type='visualbert', target_class='hate'):
        """
        Visualize where the model is focusing in the text using attention weights.
        
        Args:
            text: Input text
            inputs: Tokenized inputs
            model_type: Which model to interpret ('visualbert' or 'clip')
            target_class: Target class for interpretation ('hate' or 'anti_hate')
            
        Returns:
            Text with highlighted tokens based on attention weights
        """
        # Choose the model for interpretation
        if model_type == 'visualbert':
            model_name = "VisualBERT"
            if target_class == 'hate':
                model = self.ensemble_model.visual_model_neg.visual_bert
            else:
                model = self.ensemble_model.visual_model_pos.visual_bert
        else:  # CLIP
            model_name = "CLIP"
            if target_class == 'hate':
                model = self.ensemble_model.clip_model_neg.clip_model
            else:
                model = self.ensemble_model.clip_model_pos.clip_model
        
        # For VisualBERT, get attention weights from the model
        attention_scores = []
        tokens = []
        
        # Prepare inputs
        input_ids = inputs['input_ids'].squeeze(0).to(self.device)  # Remove batch dimension
        attention_mask = inputs['attention_mask'].squeeze(0).to(self.device)
        
        if model_type == 'visualbert':
            # Get tokens
            tokenizer = self.ensemble_model.tokenizer
            tokens = tokenizer.convert_ids_to_tokens(input_ids)
            
            # Forward pass to get attention weights
            with torch.no_grad():
                visual_embeds = self.ensemble_model.feature_extractor(
                    torch.zeros(1, 3, 224, 224).to(self.device)  # Dummy image
                ).unsqueeze(1)
                visual_attention_mask = torch.ones(visual_embeds.shape[:2], dtype=torch.long, device=self.device)
                
                outputs = model(
                    input_ids=input_ids.unsqueeze(0),  # Add batch dimension back
                    attention_mask=attention_mask.unsqueeze(0),
                    visual_embeds=visual_embeds,
                    visual_attention_mask=visual_attention_mask,
                    output_attentions=True
                )
                
                # Get attention weights from the last layer
                attention = outputs.attentions[-1]  # Shape: [batch_size, num_heads, seq_len, seq_len]
                attention = attention.mean(dim=1)   # Average over attention heads
                
                # Extract attention for text tokens (excluding visual tokens)
                text_attention = attention[0, :len(tokens), :len(tokens)]
                
                # Average of attention to calculate token importance
                token_importance = text_attention.mean(dim=1).cpu().numpy()
        else:  # CLIP
            # Get tokens
            tokenizer = self.ensemble_model.clip_tokenizer
            tokens = tokenizer.convert_ids_to_tokens(input_ids)
            
            # For CLIP, use a simpler approach as attention weights aren't directly accessible
            # Use gradient-based importance
            input_ids = input_ids.unsqueeze(0)  # Add batch dimension back
            input_ids.requires_grad = True
            
            # Forward pass
            with torch.enable_grad():
                outputs = model.text_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask.unsqueeze(0),
                    output_hidden_states=True
                )
                
                # Get embeddings
                embeddings = outputs.last_hidden_state
                
                # Use gradient of sum of embeddings as importance
                embeddings_sum = embeddings.sum()
                embeddings_sum.backward()
                
                # Token importance from gradients
                token_importance = input_ids.grad.abs().sum(dim=-1).cpu().numpy()[0]
        
        # Create visualization
        return self._visualize_text_importance(tokens, token_importance, model_name, target_class)
    
    def _get_integrated_gradients(self, model, image, input_ids, attention_mask, visual_embeds, 
                                  visual_attention_mask, model_type, target_class, num_steps=50):
        """Get attributions using Integrated Gradients."""
        # Wrapper function for forward pass
        def forward_func(img):
            if model_type == 'visualbert':
                # For VisualBERT, use the visual embeddings from the feature extractor
                vis_embed = self.ensemble_model.feature_extractor(img).unsqueeze(1)
                vis_attn_mask = torch.ones(vis_embed.shape[:2], dtype=torch.long, device=self.device)
                
                # Forward pass
                logits = model(input_ids, attention_mask, vis_embed, vis_attn_mask, None)
                return logits
            else:  # CLIP
                # For CLIP, use the preprocessed image
                text_inputs = self.ensemble_model.clip_tokenizer(
                    [text], padding=True, truncation=True, return_tensors="pt"
                ).to(self.device)
                
                # Forward pass
                logits = model(input_ids=text_inputs.input_ids, pixel_values=self.ensemble_model.preprocess_images_for_clip(img))
                return logits
        
        # Initialize IntegratedGradients
        ig = IntegratedGradients(forward_func)
        
        # Baseline: black image
        baseline = torch.zeros_like(image).to(self.device)
        
        # Compute attributions
        attributions = ig.attribute(image, baseline, n_steps=num_steps, target=0)
        
        return attributions
    
    def _get_occlusion(self, model, image, input_ids, attention_mask, visual_embeds, 
                      visual_attention_mask, model_type, target_class, sliding_window_shapes=(3, 15, 15)):
        """Get attributions using Occlusion."""
        # Wrapper function for forward pass
        def forward_func(img):
            if model_type == 'visualbert':
                # For VisualBERT, use the visual embeddings from the feature extractor
                vis_embed = self.ensemble_model.feature_extractor(img).unsqueeze(1)
                vis_attn_mask = torch.ones(vis_embed.shape[:2], dtype=torch.long, device=self.device)
                
                # Forward pass
                logits = model(input_ids, attention_mask, vis_embed, vis_attn_mask, None)
                return logits
            else:  # CLIP
                # For CLIP, use the preprocessed image
                text_inputs = self.ensemble_model.clip_tokenizer(
                    [text], padding=True, truncation=True, return_tensors="pt"
                ).to(self.device)
                
                # Forward pass
                logits = model(input_ids=text_inputs.input_ids, pixel_values=self.ensemble_model.preprocess_images_for_clip(img))
                return logits
        
        # Initialize Occlusion
        occlusion = Occlusion(forward_func)
        
        # Compute attributions
        attributions = occlusion.attribute(
            image, 
            sliding_window_shapes=sliding_window_shapes,
            strides=(3, 8, 8),
            target=0,
            baselines=torch.zeros_like(image).to(self.device)
        )
        
        return attributions
    
    def _get_gradient_shap(self, model, image, input_ids, attention_mask, visual_embeds, 
                          visual_attention_mask, model_type, target_class, n_samples=50):
        """Get attributions using GradientShap."""
        # Wrapper function for forward pass
        def forward_func(img):
            if model_type == 'visualbert':
                # For VisualBERT, use the visual embeddings from the feature extractor
                vis_embed = self.ensemble_model.feature_extractor(img).unsqueeze(1)
                vis_attn_mask = torch.ones(vis_embed.shape[:2], dtype=torch.long, device=self.device)
                
                # Forward pass
                logits = model(input_ids, attention_mask, vis_embed, vis_attn_mask, None)
                return logits
            else:  # CLIP
                # For CLIP, use the preprocessed image
                text_inputs = self.ensemble_model.clip_tokenizer(
                    [text], padding=True, truncation=True, return_tensors="pt"
                ).to(self.device)
                
                # Forward pass
                logits = model(input_ids=text_inputs.input_ids, pixel_values=self.ensemble_model.preprocess_images_for_clip(img))
                return logits
        
        # Initialize GradientShap
        shap = GradientShap(forward_func)
        
        # Baseline: black and white images
        baseline_black = torch.zeros_like(image).to(self.device)
        baseline_white = torch.ones_like(image).to(self.device)
        baselines = torch.cat([baseline_black, baseline_white], dim=0)
        
        # Compute attributions
        attributions = shap.attribute(
            image, 
            baselines=baselines,
            n_samples=n_samples,
            target=0
        )
        
        return attributions
    
    def _create_heatmap(self, image, attribution_magnitude, alpha=0.5, cmap='jet'):
        """Create a heatmap overlay on the image."""
        # Convert image from tensor to numpy array
        if torch.is_tensor(image):
            # Handle normalization and convert to numpy
            img_np = image.permute(1, 2, 0).numpy()
            
            # Denormalize image if necessary (assuming ImageNet normalization)
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            img_np = img_np * std + mean
            
            # Clip values to valid range
            img_np = np.clip(img_np, 0, 1)
        else:
            img_np = image
        
        # Resize attribution map to match image size if necessary
        if attribution_magnitude.shape[:2] != img_np.shape[:2]:
            attribution_magnitude = cv2.resize(attribution_magnitude, (img_np.shape[1], img_np.shape[0]))
        
        # Create heatmap
        heatmap = cv2.applyColorMap(np.uint8(attribution_magnitude * 255), cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0
        
        # Create overlay
        overlay = img_np * (1 - alpha) + heatmap * alpha
        
        # Create a figure with two subplots
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Display original image
        axes[0].imshow(img_np)
        axes[0].set_title('Original Image')
        axes[0].axis('off')
        
        # Display overlay
        axes[1].imshow(overlay)
        axes[1].set_title('Attribution Heatmap')
        axes[1].axis('off')
        
        plt.tight_layout()
        return fig
    
    def _visualize_text_importance(self, tokens, token_importance, model_name, target_class):
        """Visualize token importance."""
        # Normalize importance scores
        scaler = MinMaxScaler()
        normalized_importance = scaler.fit_transform(token_importance.reshape(-1, 1)).flatten()
        
        # Create a figure
        fig, ax = plt.subplots(figsize=(12, 5))
        
        # Create barplot
        sns.barplot(x=list(range(len(tokens))), y=normalized_importance, ax=ax)
        
        # Set ticks and labels
        ax.set_xticks(list(range(len(tokens))))
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        
        # Set title and labels
        ax.set_title(f'Token Importance - {model_name} for {target_class.capitalize()}')
        ax.set_xlabel('Tokens')
        ax.set_ylabel('Normalized Importance')
        
        plt.tight_layout()
        
        # Also create a text-based visualization
        highlighted_text = self._highlight_text_html(tokens, normalized_importance)
        
        return fig, highlighted_text
    
    def _highlight_text_html(self, tokens, importance, threshold=0.3):
        """Create HTML with highlighted tokens based on importance."""
        html = "<div style='font-family: monospace; line-height: 1.5; white-space: pre-wrap;'>"
        
        for token, imp in zip(tokens, importance):
            # Skip special tokens
            if token.startswith('[') and token.endswith(']'):
                continue
                
            # Determine color intensity based on importance
            if imp > threshold:
                intensity = min(255, int(imp * 255))
                html += f"<span style='background-color: rgba(255, 0, 0, {imp:.2f})'>{token}</span> "
            else:
                html += f"{token} "
        
        html += "</div>"
        return html
    
    def interpret_sample(self, image, text, inputs, output_path=None):
        """
        Interpret a single sample with both image and text visualizations.
        
        Args:
            image: Input image (torch.Tensor)
            text: Input text
            inputs: Tokenized inputs
            output_path: Path to save visualizations (optional)
            
        Returns:
            Dictionary with all visualizations
        """
        results = {}
        
        # Image interpretations
        print("Generating image interpretations...")
        for model_type in ['visualbert', 'clip']:
            for target_class in ['hate', 'anti_hate']:
                fig = self.visualize_image_attention(
                    image, text, inputs, model_type=model_type, 
                    target_class=target_class, attribution_method='gradient'
                )
                results[f'{model_type}_{target_class}_image'] = fig
                
                if output_path:
                    fig.savefig(f"{output_path}/{model_type}_{target_class}_image.png")
        
        # Text interpretations
        print("Generating text interpretations...")
        for model_type in ['visualbert', 'clip']:
            for target_class in ['hate', 'anti_hate']:
                fig, html = self.visualize_text_attention(
                    text, inputs, model_type=model_type, target_class=target_class
                )
                results[f'{model_type}_{target_class}_text_fig'] = fig
                results[f'{model_type}_{target_class}_text_html'] = html
                
                if output_path:
                    fig.savefig(f"{output_path}/{model_type}_{target_class}_text.png")
        
        return results


def add_interpretability_to_ensemble(EnsembleModel):
    """
    Extend the EnsembleModel class with interpretability methods.
    This function monkey-patches the EnsembleModel class.
    """
    # Add interpretability functionality
    def interpret(self, test_loader, num_samples=5, output_dir=None):
        """
        Create interpretability visualizations for a few test samples.
        
        Args:
            test_loader: DataLoader with test samples
            num_samples: Number of samples to interpret
            output_dir: Directory to save visualizations
            
        Returns:
            Dictionary with interpretations for each sample
        """
        interpreter = ModelInterpretability(self, self.device)
        interpretations = {}
        
        # Get a few samples from the test loader
        samples = []
        for image_path, images, texts, inputs, hate_labels, anti_hate_labels in test_loader:
            for i in range(min(len(images), num_samples - len(samples))):
                samples.append({
                    'image_path': image_path[i],
                    'image': images[i],
                    'text': texts[i],
                    'inputs': {
                        'input_ids': inputs['input_ids'][i].unsqueeze(0),
                        'attention_mask': inputs['attention_mask'][i].unsqueeze(0)
                    },
                    'hate_label': hate_labels[i].item(),
                    'anti_hate_label': anti_hate_labels[i].item()
                })
            
            if len(samples) >= num_samples:
                break
        
        # Interpret each sample
        for i, sample in enumerate(samples):
            print(f"Interpreting sample {i+1}/{len(samples)}...")
            
            # Make predictions
            with torch.no_grad():
                results = self.forward(
                    sample['image'].unsqueeze(0).to(self.device),
                    [sample['text']],
                    sample['inputs']
                )
            
            # Get interpretation
            sample_output_dir = f"{output_dir}/sample_{i+1}" if output_dir else None
            interpretations[f"sample_{i+1}"] = {
                'image_path': sample['image_path'],
                'text': sample['text'],
                'hate_label': sample['hate_label'],
                'anti_hate_label': sample['anti_hate_label'],
                'hate_prob': results['hate_probs'].item(),
                'anti_hate_prob': results['anti_hate_probs'].item(),
                'interpretations': interpreter.interpret_sample(
                    sample['image'], sample['text'], sample['inputs'], sample_output_dir
                )
            }
        
        return interpretations
    
    # Attach the method to the EnsembleModel class
    EnsembleModel.interpret = interpret


def interpret_predictions(ensemble_model, test_loader, num_samples=5, output_dir=None):
    """
    Standalone function to interpret ensemble model predictions.
    
    Args:
        ensemble_model: Trained ensemble model
        test_loader: DataLoader with test samples
        num_samples: Number of samples to interpret
        output_dir: Directory to save visualizations
        
    Returns:
        Dictionary with interpretations for each sample
    """
    # Add interpretability to the ensemble model
    add_interpretability_to_ensemble(type(ensemble_model))
    
    # Use the added method
    return ensemble_model.interpret(test_loader, num_samples, output_dir)


def demo_interpretability(test_image_path, test_text, ensemble_model, tokenizer, clip_tokenizer, device):
    """
    Demonstrate interpretability on a single example.
    
    Args:
        test_image_path: Path to test image
        test_text: Text to analyze
        ensemble_model: Trained ensemble model
        tokenizer: Tokenizer for VisualBERT
        clip_tokenizer: Tokenizer for CLIP
        device: Computation device
        
    Returns:
        Dictionary with interpretations
    """
    from PIL import Image
    from torchvision import transforms
    
    # Load and transform image
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    image = Image.open(test_image_path).convert('RGB')
    image_tensor = transform(image)
    
    # Tokenize text
    inputs = tokenizer(
        test_text,
        return_tensors="pt",
        max_length=128,
        padding='max_length',
        truncation=True,
    )
    
    # Create interpreter
    interpreter = ModelInterpretability(ensemble_model, device)
    
    # Get interpretations
    return interpreter.interpret_sample(image_tensor, test_text, inputs)


def interpret():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load your pretrained models (use the paths where you saved your best models)
    visual_model_neg = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    visual_model_pos = VisualBertForSentimentClassification("uclanlp/visualbert-nlvr2-coco-pre").to(DEVICE)
    clip_model_neg = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)
    clip_model_pos = CLIPForSentimentAnalysis("openai/clip-vit-base-patch32").to(DEVICE)

    # Load state dictionaries
    visual_model_neg.load_state_dict(torch.load('./models_trained/best_visual_model_hate.pth'))
    visual_model_pos.load_state_dict(torch.load('./models_trained/best_visual_model_anti_hate.pth'))
    clip_model_neg.load_state_dict(torch.load('./models_trained/best_clip_model_hate.pth'))
    clip_model_pos.load_state_dict(torch.load('./models_trained/best_clip_model_anti_hate.pth'))

    # Initialize feature extractor
    feature_extractor = ResNetFeatureExtractor().to(DEVICE)

    # Tokenizers
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

    # Create loss functions (same as in your original code)
    train_df = pd.read_csv('./hate/merged_final_hate_train_df.csv')

    # Calculate weights for hate and anti_hate
    num_hate = train_df['hate'].sum()
    num_not_hate = len(train_df) - num_hate
    pos_weight_hate = num_not_hate / num_hate

    num_anti_hate = train_df['anti_hate'].sum()
    num_not_anti_hate = len(train_df) - num_anti_hate
    pos_weight_anti_hate = num_not_anti_hate / num_anti_hate

    loss_fn_hate = FocalLoss(
        gamma=1.5, alpha=0.5, reduction='mean',
        pos_weight=torch.tensor([pos_weight_hate]).to(DEVICE)
    )

    loss_fn_anti_hate = FocalLoss(
        gamma=1.5, alpha=0.5, reduction='mean',
        pos_weight=torch.tensor([pos_weight_anti_hate]).to(DEVICE)
    )

    # Load datasets
    train_csv_path = './hate/merged_final_hate_train_df.csv'
    val_csv_path = './hate/merged_final_hate_validation_df.csv'
    test_csv_path = './hate/merged_final_hate_test_df.csv'

    # Define transform (same as in your original code)
    train_transform = transforms.Compose([
      transforms.RandomResizedCrop(224),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Load datasets
    train_dataset = MyDataset(train_csv_path, tokenizer, transform=train_transform)
    val_dataset = MyDataset(val_csv_path, tokenizer, transform=train_transform)
    test_dataset = MyDataset(test_csv_path, tokenizer, transform=train_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    # Try different ensemble strategies and compare results
    ensemble_types = ['average', 'weighted', 'learnable']
    results = {}

    # Create ensemble model
    ensemble = EnsembleModel(
        visual_model_neg=visual_model_neg,
        visual_model_pos=visual_model_pos,
        clip_model_neg=clip_model_neg,
        clip_model_pos=clip_model_pos,
        feature_extractor=feature_extractor,
        tokenizer=tokenizer,
        clip_tokenizer=clip_tokenizer,
        device=DEVICE,
        ensemble_type=ensemble_types[2]
    )
    ensemble.load_weights("./models/ensemble_learnable_weights.pth")

    interpretations = interpret_predictions(ensemble, test_loader, num_samples=5, output_dir='interpretations')
# Print results
    for sample_id, sample_data in interpretations.items():
        print(f"\nSample: {sample_id}")
        print(f"Text: {sample_data['text']}")
        print(f"Actual hate label: {sample_data['hate_label']}")
        print(f"Predicted hate probability: {sample_data['hate_prob']:.4f}")
        print(f"Actual anti-hate label: {sample_data['anti_hate_label']}")
        print(f"Predicted anti-hate probability: {sample_data['anti_hate_prob']:.4f}")

train()
#test()
#ensemble()
#interpret()

